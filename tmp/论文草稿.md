# 草稿

[TOC]

## 摘要

社会标签系统（STSs）允许用户创建、管理标签来组织网上资源，并且能帮助商家对用户进行一系列的推荐。社会标签系统由三个实体组成：用户、资源、标签。这些数据通常用三阶张量表示。在推荐领域中，基于Tucker分解的模型非常流行，例如高阶奇异值分解（HOSVD）等。然而，这类模型的时间复杂度是$O(n^3)$，时间消耗非常大，并且因为数据以三阶张量的表示形式进行计算，所以这类模型常常面临数据稀疏的问题。

本文使用了两种方式计算标签之间的相似度（cosine相似度和Wup相似度）将两种相似度相结合。之后运用聚类算法（层次聚类、谱聚类）将标签聚类，减小数据大小，更重要的是能够减轻数据稀疏所带来的影响。最后对新生成的数据进行张量分解与重构，得到推荐列表，并将结果与其他常用算法进行分析、比较。

**关键词**：社会标签系统、聚类、张量分解、推荐列表

## Abstract

Social tagging system (STS) allows users to create and manage tags to organize online resources, and can help businesses to make a series of recommendations for users. Social tagging system consists of three entities: user, resource and tag. These data are usually represented by third-order tensors. In the field of  recommendation, model based on Tucker decomposition is very popular, such as high order singular value decomposition (HOSVD). However, the time complexity of this kind of model is $o (n ^ 3) $ so the time consumption is very large. Because the data is calculated in the form of third-order tensor, this kind of model often comes up with the problem of data sparsity.
In this paper, we use two methods to calculate the similarity between tags (cosine similarity and wup similarity) and combine the two methods. Then we use clustering algorithm (hierarchical clustering, spectral clustering) to cluster tags, reduce the size of data, and more importantly, reduce the impact of data sparsity. Finally, the new data is decomposed and reconstructed to get the recommended list, and then the results are analyzed and compared with other common algorithms.

keywords: social tag system, clustering, tensor decomposition, recommendation list 

## 绪论

在大数据时代，越来越庞大的数据使用户难以快速地找到所需的数据。为了解决这一问题，人们提出社会标签系统（STS）。

在社会标签系统中，用户能够创建、管理标签来组织网上资源。这些标签能够揭示用户的兴趣点，帮助商家理解并挖掘潜在的信息。这些原理常用在推荐系统里，这种系统被称为基于标签的推荐系统。

本章首先着重介绍了基于标签的推荐系统的背景，然后阐述了本文研究的意义，最后对本文的框架进行了简要的介绍。

### 研究意义以及背景

大数据具有多关联、多维度、多变量的特点。用户在浏览大量的数据时，难免会迷茫，不知所措。这时，社会标签系统应需求而生。

标签通常用于用户以关键字的形式添加元数据对资源（歌曲、图片、产品等）进行注释或分类。在很多商家中，如Amazon、豆瓣等利用标签来分类信息并帮助客户分享他们。用户采用标签对资源进行标注，标签因此成为用户与资源的关系纽带。资源之间、用户之间也因此具有了间接的联系。例如，同一用户标注过的资源有一定的联系，标签重合度高的资源有类似相同的含义，标注过的标签重合度高的用户间可能有相同兴趣。

特别地，Golder、Huberman分析了协同标签系统及其动态意义[1]。Halpinet等人提出了基于协同标签系统的生成模型[2]，并认为在任何标签系统都存在三个实体：用户、资源和标签。

早期的推荐模型通常把上述三维空间拆分成多个二维空间：{用户，资源}、{用户，标签}和{标签，资源}。然而，这种方式会丢失一部分整体上的联系。因此，数据有必要用3阶张量表示。

高阶奇异值分解（HOSVD）的提出为高阶张量的分析提供了一种解决方案。1966年，Tucker提出三阶张量的多维扩展的分解模型[3]，这种分解被后人总结并延伸，最后，高阶奇异值分解的技术在文献[4]中提出。

然而，三元组的引入带来了两个问题。一是数据稀疏，二是时间复杂度大。为提高计算速度，Steffen Rendle 和 Lars Schmidt-Thieme提出成对交互张量分解模型（PITF）[5]，该模型使用了Turcker分解的特殊形式，能在线性时间内完成训练和预测。Cai 提出低阶张量分解的技术（LOTD）[6]，基于低阶多项式，用于解决数据稀疏的问题。之后，Rafailidis和Daras提出了基于张量分解和标签聚类大的模型（TFC）[7]，很好地降低数据稀疏的影响，并有效降低数据分析量，提高计算速度。

### 论文组织安排

论文整体上安排为5章，具体结构如下：

第一章，绪论。该章节系统地介绍了社会标签系统的背景知识，相关研究工作、本课题的研究意义，最后对论文的整体结构做出安排。

第二章，相关理论与基础。该章节系统地介绍了本文研究所需要的相关理论与基础。

第三章，基于谱聚类、HOSVD的推荐方法。该章节详细地介绍本文设计的推荐模型及其细节与步骤。

第四章，实验结果与分析。该章节详细地介绍本文所用的实验数据集以及相应参数设置，对其性能的评估以及与传统的推荐算法的性能的比较。

第五章，总结。总结该论文的工作以及下一步工作。

## 相关理论与基础

### 谱聚类

#### 基础图概念

$G=(V,E)$表示一个无向图，其顶点集合$V=\{v_1,v_2,...,v_n\}$。假设图G的各边有非负权重，即 $\forall e=(v_i,v_j)\in E$ ，有$w_{ij}\ge0$ 。对应的权重矩阵记为$W=(w_{ij})_{i,j=1,...n}$ 。如果$w_{ij}=0$表示顶点$v_i\,,v_j$不相邻。又因为$G$为无向图，因此有$w_{ij}=w_{ji}$ 。某点$v_i\in V$的度记为
$$
d_i=\sum_{j=1}^{n}w_{ij}.
$$
度数矩阵$D$为对角矩阵，对角元素为$d_1,d_2,...,d_n$ 。给定一个点集$A\subset V$， 我们把其补集$V \backslash A$ 记为$\overline A$ 。 定义一个指标向量$\mathbbm{1}$ $\mathbb{I}_A=(f_1,...,f_n)'\in\R^n$ ,其中如果$v_i\in A$，那么$f_i=1$ ，否则$f_i=0$。为了方便，我们用$i\in A$表示$\{i|v_i\in A\}$。其他定义如下：
$$
\begin{align}
&W(A,B):=\sum_{i\in A,j\in B}w_{ij}\\
&|A|:=A中顶点的总数\\
&vol(A):=\sum_{i\in A}d_i
\end{align}
$$

#### 非正规拉普拉斯矩阵

非正规拉普拉斯矩阵：
$$
L=D-W
$$
**性质1：**矩阵$L$满足下列条件：

+ $\forall f\in \R^n,f'Lf=\frac{1}{2}\sum_{i,j=1}^n(f_i-f_j)^2$ 。
+ $L$对称且半正定。
+ $L$的最小特征值为0，其对应特征向量为全一向量$\mathbb{I}$。
+ $L$有n个非负实数特征值$0=\lambda_1 \le \lambda_2 \le...\le \lambda_n$。

**性质2：**矩阵$L$的特征值为0的重数$k$等于图$G$的连通分量$A_1,...,A_k$的数目。

#### 正规拉普拉斯矩阵

以下两矩阵均被称为正规拉普拉斯矩阵：
$$
\begin{align}
&L_{sym}:=D^{-1/2}LD^{-1/2}=I-D^{-1/2}WD^{-1/2}\\
&L_{rw}:=D^{-1}L=I-D^{-1}W
\end{align}
$$
**性质1：**正规拉普拉斯矩阵满足以下性质：

+ $\forall f\in \R^n,\;f'L_{sym}f=\frac{1}{2}\sum_{i,j=1}^nw_{ij}\left(\frac{f_i}{\sqrt{d_i}}-\frac{f_j}{\sqrt{d_j}}\right)^2$。
+ $\lambda$ 是$L_{rw}$的一个特征值，其对应特征向量为 $u$当且仅当 $\lambda$是$L_{sym}$的特征值，其对应特征向量为 $w=D^{1/2}u$ 。
+ $\lambda$是$L_{rw}$的一个特征值，其对应特征向量为$u$当且仅当$\lambda$和$u$满足方程$Lu=\lambda Du$ 。
+ 0是$L_{rw}$一个特征值，其对应的特征向量为全一向量 $\mathbb{I}$。0是$L_{sym}$的一个特征值，其对应特征向量为 $D^{1/2}\mathbb{I}$。
+ $L_{sym}$和$L_{rw}$均为半正定的，且拥有$n$个非负实数特征值 $0=\lambda_1 \le...\le \lambda_n$。

**性质2：**矩阵$L_{rw},L_{sym}$的特征值为0的重数$k$等于图$G$的连通分量$A_1,...,A_k$的数目。

#### 谱聚类算法

$$
\begin{align}
&Unnormalized\;spectral\;slustering\\
&Input:\;Similarity\;matrix\;S\in \R^{n\times n},number\;k\;of\;clusters\;to\;construct\\
&\;\bullet\;Construct\;a\;similarity\;graph.\;Let\;W\;be\;its\;weighted\;adjacency\;matrix.\\
&\;\bullet\;Compute\;the\;unnormalized\;Laplacian\;L.\\
&\;\bullet\;Compute\;the\;first\;k\;eigenvectors\;u_1,...u_k\;of\;L.\\
&\;\bullet\;Let\;U\in \R^{n\times k}\;be\;the\;matrix\;containing\;the\;vectors\;u_1,...u_k\;as\;columns.\\
&\;\bullet\;For\;i=1,...,n\;let\;y_i\in \R^k\;be\;the\;vector\;corresponding\;to\;the\;ith\;row\;of\;U.\\
&\;\bullet\;Cluster\;the\;points\;(y_i)_{i=1,..n}\;in\;\R^k\;with\;the\;k-means\;algorithm\;into\;clusters\;C_1,...C_k.\\
&Output:\;Clusters\;A_1,...A_k\;with\;A_i= \left\{j|y_j\in C_i\right\}\\
\end{align}
$$

对于正规的拉普拉斯矩阵，有两种版本的算法：

第一种：
$$
\begin{align}
&Normalized\;spectral\;slustering\;according\;to\;Shi\;and\;Malik(2000)\\
&Input:\;Similarity\;matrix\;S\in \R^{n\times n},number\;k\;of\;clusters\;to\;construct\\
&\;\bullet\;Construct\;a\;similarity\;graph.\;Let\;W\;be\;its\;weighted\;adjacency\;matrix.\\
&\;\bullet\;Compute\;the\;unnormalized\;Laplacian\;L.\\
&\;\bullet\;Compute\;the\;first\;k\;eigenvectors\;u_1,...u_k\;of\;the\;generalized\;eigenproblem\;Lu=\lambda Du.\\
&\;\bullet\;Let\;U\in \R^{n\times k}\;be\;the\;matrix\;containing\;the\;vectors\;u_1,...u_k\;as\;columns.\\
&\;\bullet\;For\;i=1,...,n\;let\;y_i\in \R^k\;be\;the\;vector\;corresponding\;to\;the\;ith\;row\;of\;U.\\
&\;\bullet\;Cluster\;the\;points\;(y_i)_{i=1,..n}\;in\;\R^k\;with\;the\;k-means\;algorithm\;into\;clusters\;C_1,...C_k.\\
&Output:\;Clusters\;A_1,...A_k\;with\;A_i= \left\{j|y_j\in C_i\right\}\\
\end{align}
$$
第二种：
$$
\begin{align}
&Normalized\;spectral\;slustering\;according\;to\;Ng,Jordan,and\;Weiss(2002)\\
&Input:\;Similarity\;matrix\;S\in \R^{n\times n},number\;k\;of\;clusters\;to\;construct\\
&\;\bullet\;Construct\;a\;similarity\;graph.\;Let\;W\;be\;its\;weighted\;adjacency\;matrix.\\
&\;\bullet\;Compute\;the\;unnormalized\;Laplacian\;L_{sym}.\\
&\;\bullet\;Compute\;the\;first\;k\;eigenvectors\;u_1,...u_k\;of\;L_{sym}.\\
&\;\bullet\;Let\;U\in \R^{n\times k}\;be\;the\;matrix\;containing\;the\;vectors\;u_1,...u_k\;as\;columns.\\
&\;\bullet\;From\;the\;matrix\;T\in \R^{n\times k}\;from\;U\;by\;normalizing\;the\;rows\;to\;norm\;1,that\;is\;set\;t_{ij}=u_{ij}/(\Sigma_k u_{ik}^2)^{1/2}.\\
&\;\bullet\;For\;i=1,...,n\;let\;y_i\in \R^k\;be\;the\;vector\;corresponding\;to\;the\;ith\;row\;of\;U.\\
&\;\bullet\;Cluster\;the\;points\;(y_i)_{i=1,..n}\;in\;\R^k\;with\;the\;k-means\;algorithm\;into\;clusters\;C_1,...C_k.\\
&Output:\;Clusters\;A_1,...A_k\;with\;A_i= \left\{j|y_j\in C_i\right\}\\
\end{align}
$$


### HOSVD

#### 模态展开

假设$M\in {\R}^{n_1\times ...\times n_d}$ ，那么其模态k展开是一个$n_k-(N/n_k)$矩阵，其展开矩阵表示为$M_{(k)}$，其中，$N=n_1\times...\times n_d$。

例如，存在一个3阶张量$A\in{\R}^{4\times3\times2}$,其模态展开分别如下：

+ 模态1展开：
  $$
  \left[\begin{matrix}
  a_{111}&a_{121}&a_{131}&a_{112}&a_{122}&a_{132}\\
  a_{211}&a_{221}&a_{231}&a_{212}&a_{222}&a_{232}\\
  a_{311}&a_{321}&a_{331}&a_{312}&a_{322}&a_{332}\\
  a_{411}&a_{421}&a_{431}&a_{412}&a_{422}&a_{432}
  \end{matrix}\right]
  $$
  
+ 模态2展开：
  $$
  \left[\begin{matrix}
  a_{111}&a_{211}&a_{311}&a_{411}&a_{112}&a_{212}&a_{312}&a_{412}\\
  a_{121}&a_{221}&a_{321}&a_{421}&a_{122}&a_{222}&a_{322}&a_{422}\\
  a_{131}&a_{231}&a_{331}&a_{431}&a_{132}&a_{232}&a_{332}&a_{432}\\
  \end{matrix}\right]
  $$
  
+ 模态3展开：
  $$
  \left[\begin{matrix}
  a_{111}&a_{211}&a_{311}&a_{411}&a_{121}&a_{221}&a_{321}&a_{421}&a_{131}&a_{231}&a_{331}&a_{431}\\
  a_{112}&a_{212}&a_{312}&a_{412}&a_{122}&a_{222}&a_{322}&a_{422}&a_{132}&a_{232}&a_{332}&a_{432}\\
  \end{matrix}\right]
  $$
  

#### 模态积

假设$A\in {\R}^{n_1\times ...\times n_d}$ 以及 $M\in {\R}^{m_k\times n_k}$, 那么有
$$
B(\alpha_1,...,\alpha_{k-1},i,\alpha_{k+1},...\alpha_d)\\=\\\Sigma_{j=1}^{n_k}M(i,j)\cdot A(\alpha_1,...,\alpha_{k-1},j,\alpha_{k+1},...,\alpha_d)
$$
上述称为M与A之间的模态k乘积。

一般地，我们将上述形式简单地记为
$$
B=A\times_k M
$$
更多地，如果有 $B=A\times_k M$, 那么有$B_{(k)}=M\cdot A_{(k)}$。其中，$B_{(k)}$为$B$的模态k展开。

#### HOSVD

假设$A\in{\R}^{n_1\times...\times n_d}$，以及其模态k展开有SVD形式：
$$
A_{(k)}=U_k \Sigma_k V_k^T
$$
那么对$k=1:d$. $A$的高阶奇异值分解为：
$$
\begin{align}
A=&[[S;U_1,...U_d]]\\
=&S\times_1 U_1\times_2 U_2...\times_d U_d
\end{align}
$$
其中，$S=A\times_1 U_1^T\times_2 U_2^T...\times_d U_d^T$，称为核心张量。

更多地，可以利用截断HOSVD来近似原张量$A$：
$$
A_r=[[S(1:r_1,...,1:r_d);U_1(:,1:r_1),...U_d(:,1:r_d)]]\\
where\;r_i\leq n_i
$$

## 基于谱聚类、HOSVD的推荐方法

### 问题定义

定义$U=\{u_1,u_2,...,u_{|U|}\}$为用户集合，$I=\{i_1,i_2,...,i_{|I|}\}$为资源集合，以及$T=\{t_1,t_2,...,t_{|T|}\}$为标签集合。定义$w(u,i,t)$为对应三元组$（u,i,t）$的权值，在初始张量中，$w(u,i,t)=1$表示在该社会标签系统中，用户$u$给资源$i$打上了标签$t$，$w(u,i,t)=0$则表示无此关系；在张量分解重构后的张量中，$w(u,i,t)$表示用户$u$给资源$i$打上标签$t$的可能程度。

### 方法总览

### 标签聚类

#### Cosine相似度计算

标签聚类需要我们给出标签间的相似度。我们采用cosine相似度，设存在$t_1,t_2$，则两标签之间的cosine相似度为：
$$
CosSim(t_1,t_2)=\frac{\sum_{\forall f\in I_u I_i}(w_{t_1,f}\cdot w_{t_2,f})}{\sqrt{\sum_{\forall f\in  I_u I_i}(w_{t_1,f})^2}\sqrt{\sum_{\forall f\in  I_u I_i}(w_{t_2,f})^2}}
$$


#### 语义相似度计算

当实验数据稠密时，cosine相似度能给出令人满意的结果。然而，数据稀疏的情况在社会标签系统中十分常见，因此存在一种情况：标签$t_1,t_2$实际上应该同属于一个簇，但是因为其中某个标签数据的丢失而导致CosSim值的低下。因此我们有必要采用其它相似度来对其进行补充。

Zhibiao Wu和Martha Palmer认为[8]，在一个概念域中，两个概念的相似性由它们在层次结构中的关联程度定义，因此这里我们通过WordNet提供的语义信息来计算相似度：
$$
WupSim(t_1,t_2)=\frac{2\times N_3}{N_1+N_2+2\times N_3}\\
N_3=length(LCS)
$$
其中，$N_3$表示结点$t_1,t_2$的最近公共祖先（$LCS$）到根结点的距离。$N_1$，$N_2$分别表示$t_1$，$t_2$到其$LCS$的距离。

#### 混合相似度计算

在这一节中，我们结合以上两种相似度，最终的相似度计算如下：
$$
Sim(t_1,t_2)=(1-\alpha)\cdot CosSim(t_1,t_2)+\alpha \cdot WupSim(t_1,t_2)\\
\alpha \in [0,1]
$$
进一步标准化，可得：
$$
Sim(t_1,t_2)=(1-\alpha)\cdot \frac{CosSim(t_1,t_2)}{max(CosSim)}+\alpha \cdot \frac{WupSim(t_1,t_2)}{max(ConSim)}\\
\alpha \in [0,1]
$$


#### 聚类算法

这里我们采用基于正规拉普拉斯矩阵$L_{sym}$的谱聚类算法,其中的细节在第...章有讲到：
$$
\begin{align}
&Normalized\;spectral\;slustering\;according\;to\;Ng,Jordan,and\;Weiss(2002)\\
&Input:\;Similarity\;matrix\;S\in \R^{n\times n},number\;k\;of\;clusters\;to\;construct\\
&\;\bullet\;Construct\;a\;similarity\;graph.\;Let\;W\;be\;its\;weighted\;adjacency\;matrix.\\
&\;\bullet\;Compute\;the\;unnormalized\;Laplacian\;L_{sym}.\\
&\;\bullet\;Compute\;the\;first\;k\;eigenvectors\;u_1,...u_k\;of\;L_{sym}.\\
&\;\bullet\;Let\;U\in \R^{n\times k}\;be\;the\;matrix\;containing\;the\;vectors\;u_1,...u_k\;as\;columns.\\
&\;\bullet\;From\;the\;matrix\;T\in \R^{n\times k}\;from\;U\;by\;normalizing\;the\;rows\;to\;norm\;1,that\;is\;set\;t_{ij}=u_{ij}/(\Sigma_k u_{ik}^2)^{1/2}.\\
&\;\bullet\;For\;i=1,...,n\;let\;y_i\in \R^k\;be\;the\;vector\;corresponding\;to\;the\;ith\;row\;of\;U.\\
&\;\bullet\;Cluster\;the\;points\;(y_i)_{i=1,..n}\;in\;\R^k\;with\;the\;k-means\;algorithm\;into\;clusters\;C_1,...C_k.\\
&Output:\;Clusters\;A_1,...A_k\;with\;A_i= \left\{j|y_j\in C_i\right\}\\
\end{align}
$$


### 张量分解与重构

#### 张量$\mathcal{A}$的初始化构造

根据数据集中的三元组(user, item, tag)，我们可以构造初始化张量$\mathcal{A}\in \R ^{I_u\times I_i \times I_t}$, 其中，$I_u,I_i,I_t$ 分别为用户数量、资源数量和标签数量。若存在一个用户$u$在资源$i$上打上了标签$t$，那么有$\mathcal{A}[u,i,t]=1$，反之则有$\mathcal{A}[u,i,t]=0$。

#### 张量$\mathcal{A}$的展开

将张量$\mathcal{A}$在各个模态上展开，得到三个新矩阵：
$$
A_{(1)}\in\R ^{I_u \times I_i I_t}\\
A_{(2)}\in\R ^{I_i \times I_u I_t}\\
A_{(3)}\in\R ^{I_t \times I_u I_i}
$$

#### 奇异值分解

我们对上述新矩阵进行奇异值分解(SVD)，得：
$$
A_{(1)}=U^{(1)}\cdot S_1\cdot V_1^T\\
A_{(2)}=U^{(2)}\cdot S_2\cdot V_2^T\\
A_{(3)}=U^{(3)}\cdot S_3\cdot V_3^T\\
$$
为之后张量$\mathcal{S}$的构造，有三个参数$c_1,c_2,c_3$待定，分别对应矩阵$U^{(1)},U^{(2)},U^{(3)}$所保留的坐奇异向量的数量。$c_1,c_2,c_3$决定张量$\mathcal{S}$的维度，更重要的是，我们可以通过调整这三个参数来保留原矩阵相应百分比信息，突出趋势。O(i^3)

#### 核心张量$\mathcal{S}$的构造

构造核心张量$\mathcal{S}$：
$$
\mathcal{S}=\mathcal{A}\times_1{U_{c_1}^{(1)}}^T\times_2{U_{c_2}^{(2)}}^T\times_3{U_{c_3}^{(3)}}^T
$$
其中，$\mathcal{A}$是初始张量，${U_{c_1}^{(1)}}^T$是$U^{(1)}$ 的$c_1$维截断矩阵的转置矩阵，${U_{c_2}^{(2)}}^T$是$U^{(2)}$ 的$c_2$维截断矩阵的转置矩阵，${U_{c_3}^{(3)}}^T$是$U^{(3)}$ 的$c_3$维截断矩阵的转置矩阵。

#### 张量$\hat{\mathcal{A}}$的重构造

最后，张量$\hat{\mathcal{A}}$由核心张量$\mathcal{S}$与矩阵${U_{c_1}^{(1)}}^T,{U_{c_2}^{(2)}}^T,{U_{c_3}^{(3)}}^T$模态积得到：
$$
\hat{\mathcal{A}}=\mathcal{S}\times_1U_{c_1}^{(1)}\times_2U_{c_2}^{(2)}\times_3U_{c_3}^{(3)}
$$
其中，$\mathcal{S}$是核心张量，$U_{c_1}^{(1)}$是$U^{(1)}$ 的$c_1$维截断矩阵，$U_{c_2}^{(2)}$是$U^{(2)}$ 的$c_2$维截断矩阵，$U_{c_3}^{(3)}$是$U^{(3)}$ 的$c_3$维截断矩阵。

#### 张量分解算法

我们采用HOOI算法[9]进行张量分解与重构，该算法的优化目标如下：
$$
min\;||\mathcal{A}-[[\mathcal{S};A^{(1)},A^{(2)},A^{(3)}]]||
$$
其中，$\mathcal{A}\in\R^{U_n\times I_n\times T_n}$，$\mathcal{S}\in\R^{c_1\times c_2\times c_3}$，$A^{(1)}\in \R^{U_n\times c_1}$，$A^{(2)}\in \R^{I_n\times c_2}$，$A^{(3)}\in \R^{U_n\times c_3}$。

该算法的核心思路是利用交替最小二乘（ALS），根据Tucker分解交替计算核心张量$\mathcal{S}$以及各模态矩阵$U^{(1)},U^{(2)},U^{(3)}$，直到误差满足阈值条件或者达到迭代次数上限：
$$
\begin{align}
&procedure\;HOOI(\mathcal{A},c_1,c_2,c_3)\\
&\;\;\;\;initialize\;U^{(n)}\;for\;n=1,...,3\;using\;truncated\;SVD\\
&\;\;\;\;repeat\\
&\;\;\;\;\;\;\;\;for\;n=1,...,3\;do\\
&\;\;\;\;\;\;\;\;\;\;\;\;\mathcal{S}\gets \mathcal{A}\times_1 {U^{(1)}}^T...\times_{n-1} {U^{(n-1)}}^T\times_{n+1} {U^{(n+1)}}^T...\times_3 {U^{(3)}}^T\\
&\;\;\;\;\;\;\;\;\;\;\;\;U^{(n)}\gets U^{(n)}_{c_n}\gets c_n\;leading\;left\;singular\;vectors\;of\;\mathcal{S}_{(n)}\\
&\;\;\;\;\;\;\;\;end\;for\\
&\;\;\;\;until\;fit\;ceases\;to\;improve\;or\;maximum\;iterations\;exhausted\\
&\mathcal{S}\gets \mathcal{A}\times_1 U^{(1)}\times_2 U^{(2)}\times_3 U^{(3)}\\
&return\;\mathcal{S},U^{(1)},U^{(2)},U^{(3)}
\end{align}
$$

### 推荐列表的生成

通过张量分解算法得到的重构张量$\hat{\mathcal{A}}$代表着用户,资源,标签之间的联系。$\hat{\mathcal{A}}[u,i,t]$可以衡量用户u在资源i上打上标签t的概率大小。因此给定一个二元组$(user,tag)$，我们可以根据对应的$\hat{\mathcal{A}}[u,:,t]$筛选前N个资源来达到推荐的目的。

### 复杂度分析

以下为该算法的步骤：

+ 将标签聚类。
  + 计算标签间的cosine相似度和wup相似度，得到相似度矩阵。
  + 进行谱聚类。
+ 张量分解与重构。
  + 根据聚类结果进行映射，初始化张量$\mathcal{A}$   。
  + 执行HOOI算法分解张量$\mathcal{A}$，重构张量$\hat{\mathcal{A}}$。
+ 生成推荐列表。
  + 根据给出的(user, tag)二元组从张量$\hat{\mathcal{A}}$得到矩阵$\hat{\mathcal{A}}[user, :, tag]$ ，筛选前N个大的item进行推荐。

第一步的时间复杂度为$O(I^2)+O(k^3)$，其中，$k$分别为簇数。

第二步的时间复杂度为$O(I^2)+O(I^3)$。

第三步的时间复杂度为$O(I)$

## 实验结果与分析

### 数据集

本文所用的数据集来自第二届推荐系统信息异构与融合国际研讨会(HetRec 2011, http://ir.ii.uam.es/hetrec2011)所发布的Last.fm Web 2.0,与MovieLens数据集。这些数据集包含社交网络、标签和资源消耗信息。

为保证数据的质量，保证标签是有意义的、每个用户、资源、标签出现的频率大于5次，我们对数据集进行了预处理，得到以下情况的数据集：

| dataset              | $|I_u|$ | $|I_i|$ | $|I_t|$ | $|S|$ | $NNZ$ |
| -------------------- | ------- | ------- | ------- | ----- | ----- |
| hetrec2011-lastfm-2k | 100     | 100     | 255     | 10750 | 0.42%  |
| ml-20m(MovieLens)	   | 200     | 200     | 404     | 48864 | 0.30%  |

其中，$|S|$表示三元组$(u,i,t)$的数量，$NNZ$表示数据集的疏密程度，即三元组$(u,i,t)$的比例。

### 评价方法

我们采用准确率(Precision)、召回率(Recall)、F1 SCORE来评价推荐结果。

我们首先要生成用于训练的数据集$S_{train}$和用于测试的数据集$S_{test}$。对于每个用户$user$，我们随机抽取一个标签$tag$，后把所有$(user,:,tag)$形式的三元组全部抽取出来作为测试数据集$S_{test}$，剩下的数据集作为训练数据集$S_{train}$，其中$S_{train}:=S\setminus S_{test}$。之后我们在数据集$S_{train}$上训练，对于在数据集$S_{test}$中的元组$(user,tag)\in P_{S_{test}}$，我们根据用户$user$推荐前$N$个关于标签$tag$的资源$items$。
$$
\begin{align}
Prec(S_{test},N)&:=avg_{(u,t)\in P_{test}}{\frac{Top(u,t,N)\bigcap\{i|(u,i,t)\in S_{test}\}}{N}}\\
Recall(S_{test},N)&:=avg_{(u,t)\in P_{test}}{\frac{Top(u,t,N)\bigcap\{i|(u,i,t)\in S_{test}\}}{|\{i|(u,i,t)\in S_{test}\}|}}\\
F1(S_{test},N)&:=\frac{2\cdot Prec(S_{test},N)\cdot Recall(S_{test}, N)}{Prec(S_{test},N)+Recall(S_{test}, N)}
\end{align}
$$

## 实验结果与分析

我们将传统的TUCKER分解算法[10]、LOTD算法[6]、hierarchical clustering算法和本文的基于spectual clusering的算法进行比较：

![](/home/yikang/Documents/dataset/hetrec2011-lastfm-2k/subset/precision_compared.png)

![](/home/yikang/Documents/dataset/hetrec2011-lastfm-2k/subset/f1_compared.png)

![](/home/yikang/Documents/dataset/ml-20m/subset/precision_compared.png)

![](/home/yikang/Documents/dataset/ml-20m/subset/f1_score_compared.png)

有上图可以看出，推荐前1~10个资源（item）时，spectral clustering算法在准确率（Precision）上有明显的优势，推荐前1~6个资源（item）时，spectral clustering算法在（F1 Score）上有明显的优势。

## Ref

[1]	S. Golder and B. Huberman. The structure of collaborative tagging systems. In Technical Report, 2005.

[2]	H. Halpin, V. Robu, and H. Shepherd. The dynamics and semantics of collaborative tagging. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 211–220, 2007.

[3]	Tucker L R. Some mathematical notes on three-mode factor analysis. Psychometrika, 1966, 31(3): 279-311

[4]	L. d. Lathauwer, B. d. Moor, and J. Vandewalle. A multilinear singular value decomposition. SIAM Journal of Matrix Analysis and Applications, 21(4):1253–1278, 2000.

[5]	S. Rendle and L. S. Thieme. Pairwise interaction tensor factorization for personalized tag recommendation. In Proceedings of the third ACM international conference on Web search and data mining (WSDM ’10), pages 81–90, 2010

[6]	Y. Cai, M. Zhang, D. Luo, C. Ding, and S. Chakravarthy. Low-order tensor decompositions for social tagging recommendation. In Proceedings of the fourth ACM international conference on Web search and data mining (WSDM ’11), pages 695–704. ACM, 2011.

[7]	D. Rafailidis and P. Daras. The tfc model: Tensor factorization and tag clustering for item recommendation in social tagging systems. Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans, 2012.

[8]	Wu, Zhibiao and Palmer, Martha. Verbs semantics and lexical selection. Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138. 1994.

[9]	Kolda, T. G., & Bader, B. W. (2007). *SANDIA REPORT Tensor Decompositions and Applications*. *November*. 

[10]	Symeonidis, P., Nanopoulos, A., & Manolopoulos, Y. (2008). Tag recommendations based on tensor dimensionality reduction. *RecSys’08: Proceedings of the 2008 ACM Conference on Recommender Systems*, 43–50. 