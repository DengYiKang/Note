# 一些知识点的汇总

## 计算机网络

### 分层

<img src="../pic/413.jpg" alt="图片描述" style="zoom:80%;" />

### 传输控制协议TCP简介

+ 面向连接、可靠的、基于字节流的传输层通信协议
+ 将应用层的数据流分割成报文段并发送给目标节点的TCP层
+ 数据包都有序号，对方收到则发送ACK确认，未收到则重传
+ 使用校验和（？）来检验数据在传输过程中是否有误

### TCP Flags

+ URG：紧急指针标志，一般和紧急指针（16位）部分配合使用，紧急指针可以定位紧急数据的位置。
+ ACK：确认序号标志，为1表示确认，为0表示未含确认信息
+ PSH：push标志，为1指示接收方在接受到这个报文后应尽快上交给应用程序
+ RST：重置连接装置标志，用于重置由于主机崩溃或者其他原因而出现错误的连接，或者拒绝非法的报文段，或者拒绝连接
+ SYN：同步序号，用于建立连接过程。在连接请求中，SYN=1,ACK=0表示这个数据段没有使用捎带的确认域
+ FIN：finish标志，用于释放连接

### 三次握手

![](../pic/414.png)

#### 为什么需要三次握手才能建立起连接？

为了初始化Sequence Number的初始值，需要互相通知对方自己的Sequence值，这些Sequence要作为以后数据通信的序号，以保证各自接受到的数据不会因为网络波动等原因而乱序，TCP会用这些序号来拼接数据，这是前两次握手的作用。第三次握手就是client通知server自己已经收到，如果server没有收到，那么将会重试第二次握手。

### 首次握手的隐患——SYN超时

#### 问题起因分析

+ server收到client的SYN，回复SYN-ACK的时候未收到ACK确认
+ server不断重试直到超时，linux默认等待63秒才断开连接

#### SYN Flood

Dos攻击。某个恶意程序给server发一个SYN报文，发完后立即下线，那么server会默认等63秒才会断开这个连接，那么攻击者就会将server的连接队列耗尽，使得不能处理正常的连接请求。

#### 针对SYN Flood的防护措施

SYN Cookie是对TCP服务器端的三次握手协议三次握手协议作一些修改，专门用来防范SYN Flood攻击的一种手段。它的原理是，在TCP服务器收到TCP SYN包并返回TCP SYN+ACK包时，不分配一个专门的数据区，而是根据这个SYN包计算出一个cookie值。在收到TCP ACK包时，TCP服务器再根据那个cookie值检查这个TCP ACK包的合法性。如果合法，再分配专门的数据区进行处理未来的TCP连接。

#### 建立连接后，client出现故障怎么办

保活机制：

+ 向对方发送保活探测报文，如果未收到响应则继续发送
+ 尝试次数达到上限就断开连接

### TCP四次挥手

<img src="../pic/415.png" style="zoom:80%;" />

#### 为什么有TIME-WAIT状态

+ 确保有足够的时间让对方收到ACK包。
+ 避免新旧连接混淆，因为存在连接重用的可能，因此这些延迟收到的包可能会跟新的包混在一起。

#### 为什么需要四次握手才能断开连接

因为全双工，发送方和接受放都需要FIN报文和ACK报文。

而一方没有数据发送了，另一方可能还有数据没有发送，因此需要分别进行两次挥手。总共四次。

#### 服务器出现大量CLOSE_WAIT状态的原因

客户端发送FIN报文给服务器，而服务器没有发送ACK；或者服务器一直没有发送FIN报文。

最常见的原因是对方关闭socket连接，我方忙于读或写，没有及时关闭连接。

### UDP

#### 特点

+ 面向非连接
+ 不维护连接状态，支持同时向多个客户端传输相同的消息
+ 数据包报头子只有8个字节，相比TCP 20个，开销很小
+ 吞吐量不受拥挤算法的调节，只受限于数据生成速率、传输速率以及机器性能
+ 尽最大努力交付，不保证可靠交付，不需要维持复杂的链接状态表
+ 面向报文，不对应用程序提交的报文信息进行拆分或者合并

### TCP和UDP的区别

+ 面向连接vs无连接
+ 可靠性
+ 有序性
+ 速度
+ 量级，体现在头部大小

### TCP的滑动窗口

RTT和RTO：

+ RTT：发送一个数据包到收到对应的ACK，所花费的时间
+ RTO：重传时间间隔

TCP使用滑动窗口做流量控制和乱序重排。

<img src="../pic/416.png" style="zoom:80%;" />

AdvertisedWindow表示接收方还能再接受多少数据；EffectiveWindow表示发送方还能再发送多少数据。

### HTTP

超文本传输协议。

#### 特点

+ 支持客户/服务器模式（请求响应模型）
+ 简单快速，客户端向服务器传输数据时，只需要传输请求方法和路径
+ 灵活
+ 无连接。每次连接只处理一个请求，做出了应答就断开连接，节省传输时间。但是从HTTP 1.1起，默认使用长连接（与close相对应），即服务器等待一定时间才断开。
+ 无状态。

> HTTP1.1相较于1.0，引入了keep-alive。

#### 请求/响应的步骤

+ 客户端链接到Web服务器
+ 发送HTTP请求
+ 服务器接受请求并返回HTTP响应
+ 释放连接TCP连接
+ 客户端浏览器解析HTML内容

#### 在浏览器地址栏输入URL，按下回车之后的流程？

+ DNS解析（从近到远依次是浏览器缓存、系统缓存、路由器缓存、IPS服务器缓存、根域名服务器缓存，顶级域名服务器缓存）
+ TCP连接（三次握手）
+ 发送HTTP请求
+ 服务器处理请求并返回HTTP报文
+ 浏览器解析渲染页面
+ 连接结束（四次挥手）

#### HTTP状态码？

五种可能的取值：

+ 1xx：指示信息——表示请求已接受，继续处理
+ 2xx：成功——表示请求已被成功接受
+ 3xx：重定向——要完成请求必须进行更进一步的操作
+ 4xx：客户端错误——请求由语法错误或请求无法实现
+ 5xx：服务器端错误——服务器未能实现合法的请求

#### 常见的HTTP状态码

+ 200 OK：正常返回信息
+ 400 Bad Request：客户端请求有语法错误，不能被服务器所理解
+ 401 Unauthorized：请求未经授权，这个状态码必须和WWW-Authenticate报头域一起使用
+ 403 Forbidden：服务器收到请求，但是拒绝提供服务（例如，访问IP被禁止）
+ 404 Not Found：请求资源不存在，eg，输入了错误的URL
+ 500 Internal Server Error：服务器发生不可预期的错误
+ 503 Server Unavailable：服务器当前不能处理客户端的请求，一段时间后可能回复正常

#### GET请求和POST请求的区别

从三个层面来解答：

+ Http报文层面：GET将请求信息放在URL，POST信息放在报文体中，抓包还是能抓出明文账号密码等
+ 数据库层面：GET符合幂等性（一般做查询用），POST不符合
+ 其他层面：GET可以被缓存（如浏览器的浏览记录中），而POST不行

#### Cookie和Session

Cookie：

+ 是由服务器发给客户端的特殊信息（放在请求头），以文本的形式存放在客户端
+ 客户端再次请求的时候，会把cookie回复
+ 服务器接受到后，回揭西cookie生成与客户端相对应的内容

Cookie的设置以及发送过程：

<img src="../pic/417.png" style="zoom:80%;" />

Sessoin：

+ 服务器端的机制，在服务器上保存的信息
+ 解析客户端请求并操作session id，按序保存状态信息

Session的实现方式：

+ 使用cookie来实现：

  服务器给每个cookie分配一个session id，即jsessionid，然后发给客户，客户在后续请求中，将这个id携带在cookie中，发给服务端。

  <img src="../pic/418.png" style="zoom:80%;" />

+ 使用URL回写来实现：服务器发送给浏览器的链接中携带jsessionid的参数，客户端点击任何链接，都会将这个id带回到服务器。

> Tomcat支持两种，如果发现客户端支持cookie，那么就用cookie。

#### Cookie和Session的区别

+ Cookie数据存放在客户的浏览器上，Session数据放在服务器上
+ Session相比Cookie更安全，因为Cookie存放在用户本地，可以被获取并分析。
+ 如果考虑减轻服务器负担，应当使用Cookie。

### HTTP和HTTPS的区别

<img src="../pic/419.png" style="zoom:80%;" />

#### SSL(Security Sockets Layer，安全套接层)

+ 为网络通信提供安全及数据完整性的一种安全协议
+ 是操作系统对外的API，SSL3.0后更名为TLS
+ 采用身份验证和数据加密保证网络通信的安全和数据的完整性

#### 加密方式

+ 对称加密：加密和解密都使用同一个密钥

+ 非对称加密：加密使用的密钥和解密使用的密钥是不相同的

+ 哈希算法：将任意长度的信息转换为固定长度的值，算法不可逆

+ 数字签名：证明某个消息或者文件是某人发出/认同的（证明没有被修改过）

  > 数字签名：
  >
  > （1）甲使用乙的公钥对明文进行加密，生成密文信息。
  >
  > （2）甲使用HASH算法对明文进行HASH运算，生成数字指纹。
  >
  > （3）甲使用自己的私钥对数字指纹进行加密，生成数字签名。
  >
  > （4）甲将密文信息和数字签名一起发送给乙。
  >
  > （5）乙使用甲的公钥对数字签名进行解密，得到数字指纹。
  >
  > （6）乙接收到甲的加密信息后，使用自己的私钥对密文信息进行解密，得到最初的明文。
  >
  > （7）乙使用HASH算法对还原出的明文用与甲所使用的相同HASH算法进行HASH运算，生成数字指纹。然后乙将生成的数字指纹与从甲得到的数字指纹进行比较，如果一致，乙接受明文；如果不一致，乙丢弃明文。
  >
  > 
  >
  > 非对称加密和数字签名的区别：
  >
  > 对于非对称加密，**整个数据加密和解密过程用的都是接收方的密钥**，而数字签名则完全相反，**整个数据签名和解密的过程用的都是发送方的密钥**。

#### HTTPS证书

在服务器上生成CSR文件（证书申请文件，内容包括证书公钥、使用的Hash算法、申请的域名、公司名称、职位等信息），把CSR文件和其他可能的证件上传到CA认证机构，CA机构收到证书申请之后，使用申请中的Hash算法，对部分内容进行摘要，然后`使用CA机构自己的私钥对这段摘要信息进行签名`， 然后CA机构把签名过的证书通过邮件形式发送到申请者手中。申请者收到证书之后部署到自己的web服务器中。

浏览器端拥有CA的公钥，在获取到服务器发来的证书后，对签名进行解密，得到摘要1，然后对其他信息进行hash，得到摘要2，比对这两个摘要，如果不一致，那么不可信。

#### HTTPS流程

+ 浏览器将支持的加密算法信息发送给服务器
+ 服务器选择一套浏览器支持的加密算法，以证书的形式回发给浏览器
+ 浏览器验证证书合法性，然后做以下操作：
  + 定义一个握手信息，使用哈希算法进行加密，得到A
  + 生成随机的对称加密的密码，使用服务器的公钥进行加密，得到B
  + 用随机生成的密码对A进行对称加密，得到C
  + 将A，B，C发给server
+ server端进行验证，具体操作如下：
  + 使用server端的私钥对B进行解密，得到对称密钥
  + 使用上述的对称密钥对C进行解密，得到被哈希过的握手信息
  + 将这个握手信息与A进行比对，如果一致，则消息合法
+ 如果上述消息合法，server端使用上述对称密钥加密新的握手信息，发送给浏览器
+ 浏览器用对称密钥解密握手信息，然后计算握手信息的哈希值，如果与服务端发来的相同，那么消息合法
+ 之后就使用浏览器生成的随机密码和对称加密算法传输数据了

#### HTTP和HTTPS的区别

+ HTTPS需要到CA申请证书，HTTP不需要
+ HTTPS密文传输，HTTP明文传输
+ 连接方式不同，HTTPS默认443端口，HTTP使用80端口
+ HTTPS=HTTP+加密+认证+完整性保护，较HTTP安全

#### HTTPS真的安全吗

+ 浏览器默认填充http://，请求需要进行跳转，有被劫持的风险
+ 可以使用HSTS（HTTP Strict Transport Security）优化

### Socket简介

Sockt是对TCP/IP协议的抽象，方便于使用TCP/IP协议栈。是操作系统对外开放的接口。

![](../pic/421.png)

## 数据库

### 如何设计一个关系型数据库？

![](../pic/422.png)

### 索引模块

#### B+树相比于B树的优点

+ B+树的磁盘读写代价更低。因为B+数非叶子节点不存储数据，只存储键，因此每个非叶子节点存储的数据要比B树更多，对于相同的读内存的次数，读入内存的键会更很多，读写代价更低。
+ B+树的查询效率更稳定，因为非叶子节点不存储数据，所有的数据必须在叶子节点上获得，所以查询效率很稳定
+ B+树的叶子节点之间形成了链表，有利于数据库的扫描

#### Hash索引的缺点

+ 只能满足“=”，“IN”，不能满足范围查询
+ 因为是无序的，所以无法用索引来排序
+ 不能利用部分索引键查询（最左前缀，B数和B+树都支持最左前缀）
+ hash可能存在冲突，因此即使找到对应的bucket，也需要遍历冲突链表来查找

#### 位图索引

<img src="../pic/423.png" style="zoom:80%;" />

#### 密集索引和稀疏索引的区别

+ 密集索引文件中的每个搜索码值都对应一个索引值
+ 稀疏索引文件只为索引码的某些值建立索引项，比如只保存主键的信息

#### InnoDB的密集索引

+ 若一个主键被定义，该主键作为密集索引
+ 若没有主键被定义，该表的第一个唯一非空索引则作为密集索引
+ 若不满足以上条件，innodb内部会生成一个隐藏的主键做密集索引
+ 非主键索引存储相关键位和其对应的主键值，包含两次查找

### 如何定位并优化慢查询sql

#### 根据慢日志定位慢查询sql

+ `show variables like '%quer%'`，查找关于query的相关配置
  + `slow_query_log=off`（默认）表示慢日志关闭，需要开启
  + `slow_query_log_file`为慢日志存储位置(注意DDL数据定义语言不会被记录在内)
  + `long_query_time`是一个时长，查询时长超过它将会被记录在慢日志里
  + 使用`set global <name> <value>`语句更改上述配置
+ `show status like '%slow_queries%'`，显示慢查询语句的条数
+ 日志中，会显示查询时间、查询语句等

#### 使用explain等工具分析sql

在查询的语句前加上explain关键字，注意这样并不会真正地去查询数据，而是对该sql显示分析结果。

Explain关键字段：

+ type：

  ![](../pic/424.png)

+ extra：

  ![](../pic/425.png)

> 做测试的时候可以使用force index来强迫语句走指定的索引

#### 修改sql或者尽量让sql走索引

注意，走主键未必比走其他的索引要快。例如count(id)，直接走unique key要比primary key要快。因为辅助索引的叶子节点只存储主键，不存储其他数据，而count只需要统计个数，无需访问数据，因此主键索引的非叶子节点存储了其他数据而导致效率降低。

#### 最左匹配原则

+ mysql会一直向右匹配直到遇到范围查询（>, <, between, like）就停止匹配，比如a=3 and b=4 and c>5 and d=6，如果建立(a, b, c, d)顺序索引，d是用不到索引的，如果建立(a, b, d, c)则可以用到，a, b, d的顺序可以任意调整。
+ =和in可以乱序，比如a=1 and b=2 and c=3建立(a, b, c)索引可以任意顺序，mysql的查询优化器会帮助优化成索引可以识别的顺序。(也就是说，查询语句可以被优化，但是索引的顺序是固定的，无法被优化)

### 索引是建立得越多越好吗

+ 数据量小的表不需要建立索引，建立会增加额外的索引开销
+ 数据变更需要维护索引，因此更多的索引意味着更多的维护成本
+ 更多的索引意味着也需要更多的空间

### MyISAM与InnoDB关于锁方面的区别是什么？

+ MyISAM默认用的是表级锁，不支持行级锁
+ InnoDB默认用的是行级锁，也支持表级锁

对于MyISAM和InnoDB，select语句默认会加上读锁，是共享锁。当然也可以使用select for update加上排他锁。

使用select lock in share mode显式地加上共享锁。

对于InnoDB来说，如果sql没有走索引，那么会加上表级锁，而非行锁。

#### 锁的粒度越小越好？

粒度越小，加锁的代价越高。例如表锁只需要加载头结点，而行锁需要找到对应的行才能加锁。

### MyISAM适合的场景

+ 频繁执行全表count语句，因为MyISAM内部用了一个变量保存了这个统计信息。
+ 对数据进行增删改的频率不高，查询非常频繁
  + InnoDB要维护的东西多，例如维护MVCC等
  + InnoDB寻址要映射到块，再到行，MyISAM记录的是文件的offset，定位比InnoDB快
+ 没有事务

### InnoDB适合的场景

+ 数据增删改查都相当频繁
+ 可靠性要求非常高，要求支持事务

### MySQL乐观锁和悲观锁

乐观锁就是从应用层面上做并发控制。在数据库表中增加版本字段（或者时间戳），妹修改成功一次，版本号+1，每一次对数据的更新时带上原先获取的版本号，如果版本号变了，那么where条件自然不符合。

update t_goods  set status=2,version=version+1  where id=#{id} and version=#{version};

> 其实不需要版本号也能实现乐观锁，where条件中判断status是否是之前的status即可。这里引入版本号是为了解决ABA的问题。

> 虽说在应用层面避免了加锁，但是数据库层面上，update语句还是会自动加锁的。

悲观锁就是从数据库层面上做并发控制。

### 数据库事务的四大特性

#### ACID

+ 原子性（Atomic）
+ 一致性（Consistency）
+ 隔离性（Isolation）
+ 持久性（Durability）

### InnoDB可重复读隔离级别下如何避免幻读

+ 表象：快照读（非阻塞读）——伪MVCC

  > 为什么“伪”，因为undo log只是串行化的版本，并不是真正意义上的多版本。

+ 内在：next-key锁（行锁+gap锁）

### 当前读和快照读

select ... lock in share mode， select ... for update， update，delete，insert等均为当前读。

不加锁的非阻塞读，普通的select为快照读。

在RC的级别下，快照读和当前读的结果相同。

### next-key锁（行锁+gap锁）

#### 对主键索引或者唯一索引会用Gap锁吗

+ 如果where条件全部命中，则不会用Gap锁，只会加记录锁

  > 例如，where in (1, 2, 3)，如果123都存在，则不会加Gap锁，如果只存在其中的部分，那么会加Gap锁

+ 如果where条件部分命中或者全不命中，则会加Gap锁

#### Gap锁会用在非唯一索引或者不走索引的当前读中

+ 非唯一索引
+ 不走索引将锁住所有区间

#### Gap锁是锁在索引的指针上的

例如字段name为主键，id为非唯一索引，有如下数据：

| id   | 2    | 6    | 9    | 9    | 11   | 15   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| name | h    | c    | b    | d    | f    | a    |

session A执行下列操作：

```sql
delete from tb where id = 9;
```

session A还未提交，那么因为id为非唯一索引，因此不能保证其他session同时插入id为9的数据，需要在(6, c)到(9, d)之间加gap锁。（如果id是唯一索引或主键，那么不需要加gap锁，因为唯一约束保证其他session不会插入id为9的数据）。

但是要注意的是，gap锁的实现是加在链表上的next指针上的，因此其他session还是能插入(6, b)的！因为id相同，那么按照name排序，因此(6, b)在(6, c)的左侧，不受gap锁的影响，同理，(6, d)在(6, c)的右侧，因为gap锁而无法插入！

## Redis

### 缓存中间件——Memcache和Redis的区别

Memcache：

+ 支持简单数据类型
+ 不支持数据持久化存储
+ 不支持主从
+ 不支持分片

Redis：

+ 数据类型丰富
+ 支持数据磁盘持久化存储
+ 支持主从
+ 支持分片

### Redis多路I/O复用模型

Redis采用的I/O多路复用函数：(linux)epoll/(freebsd)kqueue/evport/select?

+ 因系统不同而不同。
  + linux中的epoll
  + macos/freebsd中的kqueue
  + Solaries中的evport
  + select在不同的版本的系统上都会实现，作为保底方案
+ 优先选择时间复杂度为$O(1)$的I/O多路复用函数作为底层实现。
+ 以时间复杂度为$O(n)$的select作为保底方案。
+ 基于react设计模式监听I/O事件

### 如何从海量Key查询出某一固定前缀的Key？

#### 使用keys对线上业务的影响？

KEYS pattern：查找所有符合给定模式pattern的key。

+ keys指令一次性返回所有匹配的key
+ 键的数量过大会使服务卡顿
+ 消耗大量的内存

因此不推荐使用keys。

推荐使用scan指令。

```
SCAN cursor [MATCH pattern] [COUNT count]
```

+ 基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程
+ 以0为游标开始一次新的迭代，直到命令返回游标0完成一次遍历
+ 不保证每次执行都返回某个给定数量的元素，支持模糊查询
+ 一次返回的数量不可控，只能大概率符合count参数

```
scan 0 match k1* count 10

output:
1) "11545336"
2) 1) "k123124"
   2) "k324324"
   3) "k343432"
```

其中，返回的第一行是游标，后面的是结果集。

> 注意，cursor不一定是递增的，因此每次scan可能得到相同的key值，因此需要判重。

### 使用Redis实现分布式锁，SETNX和EXPIRE的组合非原子性问题

虽然命令是原子性的，但是组合并不是原子性的，如果程序执行完SETNX指令，挂掉了，没有执行EXPIRE指令，应该怎么解决？

Redis2.6.12版本后提供了新的命令，使用set操作将上述两个功能组合在一起：

SET key value [EX second] [PX milliseconds] [NX|XX]

+ EX second：设置键的过期时间为second秒
+ PX milliseconds：设置键的过期时间为milliseconds毫秒
+ NX：只有当键不存在时，才对键进行设置操作
+ XX：只有在键已经存在时，才对键进行设置操作
+ SET操作成功时返回OK，否则返回NIL（注意这里返回的不是1或0）

### 大量的key同时过期的注意事项

集中过期，由于清除大量的key很耗时，会出现短暂的卡顿现象。并且如果有大量的流量涌入，因为缓存大量过期，会直接走数据库，对数据库造成压力。

解决方案：设计过期时间为随机时间

### 如何使用Redis做异步队列

+ 使用List作为队列，RPUSH生产消息，LPOP消费消息。
  + 缺点：LPOP不会等待队列里有值才会去消费
  + 弥补：应用层加入Sleep机制去调用LPOP重试

+ 当然也有BLPOP指令来解决上述缺陷
  + BLPOP key [key ...] timeout：阻塞直到队列有消息或者超时
  + 缺点：只能提供给一个消费者消费

+ pub/sub：主题订阅者模式

  + 发送者（pub）发送消息，订阅者（sub接受消息）

  + 订阅者可以订阅任意数量的频道

    ```shell
    subcribe topic
    ```

    ```shell
    publish topic "message"
    ```
    
  + 缺点：消息的发布是无状态的，无法保证可达。如果某个消费者下线了，那么下线的这段时间发布的消息他无法收到。
  

### 使用pipeline的好处

Redis基于请求/响应模型，单个请求处理需要一一应答。

PipeLine批量执行指令，节省多次IO往返的时间。

有顺序依赖的指令建议按顺序分批发送。

### Redis集群原理

#### 如何从海量数据里快速找到所需？

+ 分片：按照某种规则去划分数据，分散存储在多个节点上
+ 常规的按照哈希划分无法实现节点的动态增减
+ 解决：一致性哈希算法

#### 一致性哈希算法

对2^32取模，将哈希值空间组织成虚拟的圆环。

服务器节点分布在这个hash环上，对于某个数据，计算该数据的hash值，然后在换上从这个hash值出发，顺时针找到第一个服务器，这个服务器就是数据分布的位置。

如果需要动态添加节点C到A和B之间，那么只会影响AC之间的数据分布，而CB之间的数据分布不会收到影响，因此尽可能地降低数据分布的变化。

但是存在一个问题，那就是数据倾斜的问题，某段弧的长度远远大于其他弧。解决的方法是引入虚拟节点来填充那些空，使得数据分散在虚拟节点上。一个物理节点对应多个虚拟节点。

## Linux知识

### Linux体系结构

体系结构主要分为用户态和内核态。

内核本质是一段管理计算机硬件设备的程序。

系统调用：内核的访问接口，是一种原子操作。

公用函数库：系统调用的封装。

Shell：命令解释器，可编程。

### 查找特定的文件

可以使用man指令查看使用说明

```bash
man find
```

```bash
find [path] [-option] params
# -iname忽略大小写
find -iname "file"
```

### 检索文件内容

```bash
# 查找文件里符合条件的字符串
# 如果不指定file，那么默认从输入流中读取
grep [options] pattern file
```

```bash
# 从target打头的文件中查找含有moo的行
grep "moo" target*
```

管道操作符：

可将指令连接起来，前一个指令的输出作为后一个的输入。

```bash
# 后面的grep没有指定file，因此从输入流读取
# 因为用了管道，因此grep是从find ~的结果中读取
# 功能为查找所有文件中含有“target”的行
find ~ | grep "target"
# keyword为关键词，pattern为过滤的正则，先从file中查找包含关键词的行，然后过滤出想要的信息，比如错误日志中出错的ip之类的
grep keyword file | grep -o pattern
# 查找tomcat的进程
ps -ef | grep tomcat
# grep -v用于过滤信息，因为grep命令也包含tomcat字段，因此-v会过滤掉这个进程
ps -ef | grep tomcat | grep -v "grep"
```

> 注意：
>
> + 不处理错误输出。前一个命令的错误输出不会作为后面的输入。
> + 右边命令必须能够接受标准输入流，否则传递过程中数据会被抛弃。
> + 常用的接受标准输入流的命令：sed, awk, grep, cut, head, top, less, more, wc, join, sort, split等。

### 对日志内容做统计

```bash
awk [options] 'cmd' file
```

+ 一次读取一行文本，按输入分隔符进行切片，切成多个组成部分
+ 将切片直接保存在内建的变量中，\$1, \$2...（\$0表示行的全部）
+ 支持对单个切片的判断，支持循环判断，默认分隔符为空格

```shell
# 对netstat.txt文件每行进行切片（默认分隔符为空格），然后输入第一个切片和第4个切片
awk 'print $1, $4' netstat.txt
# 也可以添加过滤条件
awk '$1=="tcp" && $2==1 {print $0}' netstat.txt
# 把表头也打印出来
awk '($1=="tcp" && $2==1) || NR==1 {print $0}' netstat.txt
# 可以使用-F指定其他分隔符
awk -F "," '{print $2}' file
# 统计个数，arr[i]中的i为切片内容，arr[i]为i的出现个数
awk '{arr[$1]++}END{for(i in arr)print i "\t" arr[i]}'
```

### 批量替换文本内容

sed：

全名stream editor，流编辑器，适合用于对文本的行内容进行处理。

```bash
sed [option] 'sed command' filename
# -i是指将替换后的文本覆盖原文本，默认是将替换后的文本输出到控制台，而不是覆盖原文本
# s表示字符串操作，/.../.../，一二两个斜杠之间的pattern将会被二三两个斜杠间的字符串替换
sed -i 's/^Str/String/' filename
# 默认情况下，替换首个匹配到的字符串，如果需要全局匹配，需要加上g
sed -i 's/^Str/String/g' filename
# 删除匹配的关键词的行（注意是包含pattern的一整行）
sed -i '/pattern/d' filename
```

