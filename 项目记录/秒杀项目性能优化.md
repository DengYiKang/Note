# 秒杀项目性能优化

## 打包与部署

maven打包命令：

```shell
mvn clean package -Dmaven.test.skip=true
```

但是这有个问题，得到的jar包里面并没有包含依赖库，且执行报错，显示找不到主清单文件。

在pom文件里添加plugin：

```xml
<plugins>
    <plugin>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-maven-plugin</artifactId>
    </plugin>
</plugins>
```

> 如果标红，可以先指定一个version，然后再把version删除。

然后再用maven打包即可。

在很多场景下需要变换不同的配置信息，在已经打好包的情况下不方便再次更改application.properties源码，可以使用外挂配置：

```shell
java -jar miaosha.jar --spring.config.addition-location=application.properties
```

编写sh脚本：

```sh
nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar miaosha.jar --spring.config.addition-location=/www/miaosha/application.properties &
```

## PV, TPS, QPS

+ PV=page view，指页面被浏览的次数，比如你打开一网页，那么这个网站的pv就算加了一次；
+ TPS=transactions per second，每秒内的事务数，比如执行了dml操作，那么相应的tps会增加；
+ QPS=queries per second，每秒内查询次数，比如执行了select操作，相应的qps会增加。
+ RPS=requests per second

### TPS与QPS是有区别的

事务表示客户端发起请求到收到服务端最终响应的整个过程，这是一个TPS。

而在这个TPS中，为了处理第一次请求可能会引发后续多次对服务端的访问才能完成这次工作，每次访问都算一个QPS。

**所以，一个TPS可能包含多个QPS**。

## 性能压测

### Jmeter

使用Jmeter压测工具。

更改字体大小和编码，编辑bin/jmeter.properties：

将以下两个开启：

```properties
jsyntaxtextarea.font.family=Hack
jsyntaxtextarea.font.size=25
```

注意，开启完后需要设置主题为system才能生效。

更改编码：

```properties
sampleresult.default.encoding=UTF-8
```

可以将云服务器的主机加入到/etc/hosts中，方便测试。

开始测试：

![](../pic/81.png)

![](../pic/82.png)

### 基础命令

#### pstree

pstree命令是用于查看进程树之间的关系，即哪个进程是父进程，哪个是子进程，可以清楚的看出来是谁创建了谁

几个重要的参数：

+ -A: 各进程树之间的连接以ASCII码字符来连接

+ -U:各进程树之间的连接以utf8字符来连接，某些终端可能会有错误

+ -p:同时列出每个进程的PID

+ -u: 同时列出每个进程的所属账号名称：

例如查看miaoshaserver的服务器进程：

![](/home/yikang/Document/gitRep/Note/pic/84.png)

使用管道查看线程总数：

![](/home/yikang/Document/gitRep/Note/pic/85.png)

#### top

top -H采用线程模式。

![](../pic/86.png)

#### load average（系统平均负载）

- 系统平均负载是处于runnable或uninterruptable状态的进程数。R+D状态的进程数。
- 处于runnable状态的进程，正在使用CPU或正在等待使用CPU。
- 处于uninterruptable状态的进程，正在等待某些I/O访问，比如等待磁盘。
- 平均负载没有针对系统中CPU的数量进行归一化，因此平均负载为1表示单个CPU系统始终处于满载状态，而在4 CPU系统上则意味着75％的时间处于空闲状态。

从man资料可以看出，实际上系统平均负载包括了（R+D）状态的进程。所以，**Load avaerage与CPU使用率并不是完全同步的**：

- **CPU密集型进程**——使用大量CPU会导致平均负载升高，此时这两者是一致的；（大量浮点运算或乘除运算等）
- **I/O密集型进程**——等待I/O也会导致平均负载升高，但CPU使用率不一定高；（可能Load average虚高但是CPU不忙）
- **大量等待CPU的进程调度**也会导致平均负载升高，此时的CPU使用率也会比较高。

### 发现并发容量问题

#### 基本问题

使用Jmeter提高线程数，发现错误率飙升，用pstree -p pid | wc -l发现线程数并没有很多。

#### 默认内嵌Tomcat配置

SpringBoot的自动配置参数可以查看[spring-configuration-metadata.json](/home/yikang/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/2.4.3/spring-boot-autoconfigure-2.4.3.jar!/META-INF/spring-configuration-metadata.json)。以下参数是关键信息：

+ **server.tomcat.accept-count**：等待队列长度，默认为100
+ **server.tomcat.max-connections**：最大可被连接数，默认为8192
+ **server.tomcat.threads.max**：最大工作线程数，默认为200
+ **server.tomcat.threads.min-spare**：最小工作线程数，默认为10

因此，默认配置下，连接超过8192后出现拒绝连接的情况；触发的请求超过200+100后拒绝处理。

现在我们编写application.properties，修改如下：

```properties
server.port=80
server.tomcat.accept-count=1000
server.tomcat.threads.max=300
server.tomcat.threads.min-spare=50
```

我们将等待队列的长度增加到1000，将最大工作线程数增加到300，将最小工作线程数增加到50，然后杀死进程重新启动，再使用pstree -p pid | wc -l命令发现线程数由25增加到65。当时用jmeter压测时（线程数1000），发现线程由之前的218增加到了318。

> 压测的结果相差不大。。。。。。

#### 定制化内嵌Tomcat开发

##### KeepAlive

要明确我们谈的是**TCP**的 **`KeepAlive`** 还是**HTTP**的 **`Keep-Alive`**。TCP的KeepAlive和HTTP的Keep-Alive**是完全不同的概念，不能混为一谈**。

+ TCP的**keepalive**是侧重在保持客户端和服务端的连接，一方会不定期发送心跳包给另一方，当一方断掉的时候，没有断掉的定时发送几次**心跳包**，如果间隔发送几次，对方都返回的是RST，而不是ACK，那么就释放当前链接。设想一下，如果tcp层没有keepalive的机制，一旦一方断开连接却没有发送FIN给另外一方的话，那么另外一方会一直以为这个连接还是存活的，几天，几月。那么这对服务器资源的影响是很大的。
+ HTTP的**keep-alive**一般我们都会带上中间的**横杠**，普通的http连接是客户端连接上服务端，然后结束请求后，由客户端或者服务端进行http连接的关闭。下次再发送请求的时候，客户端再发起一个连接，传送数据，关闭连接。这么个流程反复。但是一旦客户端发送connection:keep-alive头给服务端，且服务端也接受这个keep-alive的话，两边对上暗号，这个连接就可以复用了，一个http处理完之后，另外一个http数据直接从这个连接走了。减少新建和断开TCP连接的消耗。

二者的作用简单来说：

+ HTTP协议的Keep-Alive意图在于短时间内连接复用，希望可以短时间内在同一个连接上进行多次请求/响应。
+ TCP的KeepAlive机制意图在于保活、心跳，检测连接错误。当一个TCP连接两端长时间没有数据传输时(通常默认配置是2小时)，发送keepalive探针，探测链接是否存活。

##### 定制化

+ keepAliveTimeOut：多少毫秒后不响应的断开keepalive
+ maxKeepAliveRequests：多少次请求后keepalive断开失效
+ 使用`WebServerFactoryCustomizer<ConfigurableServletWebServerFactory>`定制化内嵌Tomcat配置

##### 配置类

```java
/**
 * 当Spring容器中没有TomcatEmbeddedServletContainerFactory这个bean时，会把此bean加载到Spring容器中
 */
@Component
public class WebServerConfiguration implements WebServerFactoryCustomizer<ConfigurableServletWebServerFactory> {
    @Override
    public void customize(ConfigurableServletWebServerFactory factory) {
        //使用对应工厂类提供给我们的接口定制化我们的tomcat connector
        ((TomcatServletWebServerFactory) factory).addConnectorCustomizers(new TomcatConnectorCustomizer() {
            @Override
            public void customize(Connector connector) {
                Http11NioProtocol protocol = (Http11NioProtocol) connector.getProtocolHandler();

                //定制化keepalivetimeout,设置30秒内没有请求则服务端自动断开keepalive链接
                protocol.setKeepAliveTimeout(30000);
                //当客户端发送超过10000个请求则自动断开keepalive链接
                protocol.setMaxKeepAliveRequests(10000);
            }
        });
    }
}
```

> 在customize方法里，可以利用Http11NioProtocol来设置各种参数，包括之前提到的默认内嵌Tomcat配置等。

#### 单Web容器上限

+ 线程数量：4核CPU 8G内存单进度调度线程数800~1000以上后将花费大量的时间在CPU调度上
+ 等待队列长度：队列做缓冲池用，但也不能无限长，消耗内存，出队入队也耗费CPU

#### MySql数据库QPS容量问题

+ 主键查询：千万级别数据=1~10毫秒
+ 唯一索引查询：千万级别数据=10~100毫秒
+ 非唯一索引查询：千万级别数据=100~1000毫秒
+ 无索引：百万条数据=1000毫秒+

#### MySql数据库TPS容量问题

+ 非插入更新删除操作：同查询，看where字段走的是哪种查询
+ 插入操作：1w~10w tps（依赖配置优化）

### 单机容量问题（水平扩展）

以下为解决方案：

<img src="../pic/90.jpg" style="zoom:67%;" />

我们将nginx服务器用于三个方面：

+ 作为web服务器
+ 作为动静分离服务器
+ 作为反向代理服务器

对于前端的请求有两种，一种是静态资源的请求，一种是Ajax动态请求。对于静态资源的请求交由nginx服务器来处理，即静态资源放在nginx服务器上；对于动态请求，通过nginx反向代理来均衡到两台服务器上，这两台服务器处理处理这些动态请求，并且数据库的操作交由另一台数据库服务器来处理。

## 分布式扩展

### 配置nginx

这里使用的是openresty，将压缩包下载并解压。

首先安装必要的依赖：

```shell
yum install pcre-devel openssl-devel gcc curl
```

后进行安装：

```shell
./configure
make
make install
```

之后openresty会安装到/usr/local/openresty目录下。

在/usr/local/openresty/nginx目录下启动nginx测试：

```shell
sbin/nginx -c conf/nginx.conf
```

> 修改配置后可以使用`sbin/nginx -s reload`无缝重启

可以查看conf/nginx.conf，端口为80，用浏览器访问浏览器，有以下结果：

![](../pic/91png)

### nginx Web服务器

将前端代码scp到nginx服务器上的/usr/local/openresty/nginx/html目录下，我们在nginx.conf的文件中发现以下代码：

```
server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   html;
            index  index.html index.htm;
        }
        #省略
}
```

有以下信息：

+ location节点path：指定url映射key
+ location节点内容：root指定location path后对应的根路径，index指定默认的访问页

nginx服务器监听在80端口上，将/映射到html文件夹下，因此在浏览器中直接输入nginx/login.html就能访问到我们之前拷贝到服务器上的前端代码了：

![](../pic/92.png)

根据之前的架构图，我们应该将/resources定位到nginx目录的磁盘上的静态文件，因此需要修改nginx.conf文件：

修改如下，将/resources/路径映射到本地的nginx目录下的/html/resources/上，因此需要新建一个resources文件夹，然后将所有的静态文件转移到/resources目录下。

```
location /resources/ {
	alias       /usr/local/openresty/nginx/html/resources/;
	index  index.html index.htm;
}
```

使用新的路径访问：miaoshaserver/resources/login.html：

![](../pic/94.png)

### nginx 动静分离服务器

#### nginx配置

修改nginx.conf文件：

    upstream backend_server{
        server miaoshaserver1 weight=1;
        server miaoshaserver2 weight=1;
    }
    
    server {
        listen       80;
        server_name  localhost;
    
        #charset koi8-r;
    
        #access_log  logs/host.access.log  main;
    
        location /resources/ {
            alias       /usr/local/openresty/nginx/html/resources/;
            index  index.html index.htm;
        }
    
        location / {
            proxy_pass http://backend_server;
            proxy_set_header Host $http_host:$proxy_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
    }
首先添加**upstream**节点，设置权重**weight**。然后在/resources/的location节点后面再添加一个location节点（注意先后顺序，这将影响匹配的节点）。

**proxy_pass**字段填写需要转发的地址，**proxy_set_header**字段用于设置请求头。

如果后端服务器想知道用户的真实IP（因为默认下是nginx代理服务器发请求到后端服务器，后端服务器只能知道代理服务器的IP），我们必须配置**proxy_set_header**字段：

+ **proxy_set_header Host \$http_host:\$proxy_port**：

  设置Host为用户的真实IP与端口。

+ **proxy_set_header X-Real-IP $remote_addr**：

  X-Real-IP 代表的是客户端请求真实的 IP 地址。

+ **proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for**：

  X-Forwarded-For 记录着从客户端发起请求后访问过的每一个 IP 地址，当然第一个是发起请求的客户端本身的地址，各 IP 地址间由“英文逗号+空格”(`,`)分隔。

然后重启nginx服务器。

#### 后端服务器开启日志

在两个后端服务器的/www/miaosha路径下新建文件夹tomcat存放log日志。

在application.properties添加以下配置：

```properties
server.tomcat.accesslog.enabled=true
server.tomcat.accesslog.directory=/www/miaosha/tomcat
server.tomcat.accesslog.pattern=%h %l %u %t "%r" %s %b %D
```

pattern的含义：

+ `%h`：远端请求的IP地址
+ `%l`：
+ `%u`：远端主机的user
+ `%t`：时间
+ `"%r"`：http请求的第一行，例如：GET /item/list HTTP/1.0
+ `%s`：http返回状态码
+ `%b`：response的大小
+ `%D`：处理时长

例如：

```
172.22.228.146 - - [31/Mar/2021:21:50:39 +0800] "GET /item/list HTTP/1.0" 200 883 1089
```

#### 验证

现在可以访问浏览器，在两个后端服务器的tomcat日志中可以看到各自被转发的次数。





