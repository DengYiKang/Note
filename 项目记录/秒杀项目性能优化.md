# 秒杀项目性能优化

## 打包与部署

maven打包命令：

```shell
mvn clean package -Dmaven.test.skip=true
```

但是这有个问题，得到的jar包里面并没有包含依赖库，且执行报错，显示找不到主清单文件。

在pom文件里添加plugin：

```xml
<plugins>
    <plugin>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-maven-plugin</artifactId>
    </plugin>
</plugins>
```

> 如果标红，可以先指定一个version，然后再把version删除。

然后再用maven打包即可。

在很多场景下需要变换不同的配置信息，在已经打好包的情况下不方便再次更改application.properties源码，可以使用外挂配置：

```shell
java -jar miaosha.jar --spring.config.addition-location=application.properties
```

编写sh脚本：

```sh
nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar miaosha.jar --spring.config.addition-location=/www/miaosha/application.properties &
```

## PV, TPS, QPS

+ PV=page view，指页面被浏览的次数，比如你打开一网页，那么这个网站的pv就算加了一次；
+ TPS=transactions per second，每秒内的事务数，比如执行了dml操作，那么相应的tps会增加；
+ QPS=queries per second，每秒内查询次数，比如执行了select操作，相应的qps会增加。
+ RPS=requests per second

### TPS与QPS是有区别的

事务表示客户端发起请求到收到服务端最终响应的整个过程，这是一个TPS。

而在这个TPS中，为了处理第一次请求可能会引发后续多次对服务端的访问才能完成这次工作，每次访问都算一个QPS。

**所以，一个TPS可能包含多个QPS**。

## 性能压测

### Jmeter

使用Jmeter压测工具。

更改字体大小和编码，编辑bin/jmeter.properties：

将以下两个开启：

```properties
jsyntaxtextarea.font.family=Hack
jsyntaxtextarea.font.size=25
```

注意，开启完后需要设置主题为system才能生效。

更改编码：

```properties
sampleresult.default.encoding=UTF-8
```

可以将云服务器的主机加入到/etc/hosts中，方便测试。

开始测试：

![](../pic/81.png)

![](../pic/82.png)

### 基础命令

#### pstree

pstree命令是用于查看进程树之间的关系，即哪个进程是父进程，哪个是子进程，可以清楚的看出来是谁创建了谁

几个重要的参数：

+ -A: 各进程树之间的连接以ASCII码字符来连接

+ -U:各进程树之间的连接以utf8字符来连接，某些终端可能会有错误

+ -p:同时列出每个进程的PID

+ -u: 同时列出每个进程的所属账号名称：

例如查看miaoshaserver的服务器进程：

![](/home/yikang/Document/gitRep/Note/pic/84.png)

使用管道查看线程总数：

![](/home/yikang/Document/gitRep/Note/pic/85.png)

#### top

top -H采用线程模式。

![](../pic/86.png)

#### load average（系统平均负载）

- 系统平均负载是处于runnable或uninterruptable状态的进程数。R+D状态的进程数。
- 处于runnable状态的进程，正在使用CPU或正在等待使用CPU。
- 处于uninterruptable状态的进程，正在等待某些I/O访问，比如等待磁盘。
- 平均负载没有针对系统中CPU的数量进行归一化，因此平均负载为1表示单个CPU系统始终处于满载状态，而在4 CPU系统上则意味着75％的时间处于空闲状态。

从man资料可以看出，实际上系统平均负载包括了（R+D）状态的进程。所以，**Load avaerage与CPU使用率并不是完全同步的**：

- **CPU密集型进程**——使用大量CPU会导致平均负载升高，此时这两者是一致的；（大量浮点运算或乘除运算等）
- **I/O密集型进程**——等待I/O也会导致平均负载升高，但CPU使用率不一定高；（可能Load average虚高但是CPU不忙）
- **大量等待CPU的进程调度**也会导致平均负载升高，此时的CPU使用率也会比较高。

### 发现并发容量问题

#### 基本问题

使用Jmeter提高线程数，发现错误率飙升，用pstree -p pid | wc -l发现线程数并没有很多。

#### 默认内嵌Tomcat配置

SpringBoot的自动配置参数可以查看[spring-configuration-metadata.json](/home/yikang/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/2.4.3/spring-boot-autoconfigure-2.4.3.jar!/META-INF/spring-configuration-metadata.json)。以下参数是关键信息：

+ **server.tomcat.accept-count**：等待队列长度，默认为100
+ **server.tomcat.max-connections**：最大可被连接数，默认为8192
+ **server.tomcat.threads.max**：最大工作线程数，默认为200
+ **server.tomcat.threads.min-spare**：最小工作线程数，默认为10

因此，默认配置下，连接超过8192后出现拒绝连接的情况；触发的请求超过200+100后拒绝处理。

现在我们编写application.properties，修改如下：

```properties
server.port=80
server.tomcat.accept-count=1000
server.tomcat.threads.max=300
server.tomcat.threads.min-spare=50
```

我们将等待队列的长度增加到1000，将最大工作线程数增加到300，将最小工作线程数增加到50，然后杀死进程重新启动，再使用pstree -p pid | wc -l命令发现线程数由25增加到65。当时用jmeter压测时（线程数1000），发现线程由之前的218增加到了318。

> 压测的结果相差不大。。。。。。

#### 定制化内嵌Tomcat开发

##### KeepAlive

要明确我们谈的是**TCP**的 **`KeepAlive`** 还是**HTTP**的 **`Keep-Alive`**。TCP的KeepAlive和HTTP的Keep-Alive**是完全不同的概念，不能混为一谈**。

+ TCP的**keepalive**是侧重在保持客户端和服务端的连接，一方会不定期发送心跳包给另一方，当一方断掉的时候，没有断掉的定时发送几次**心跳包**，如果间隔发送几次，对方都返回的是RST，而不是ACK，那么就释放当前链接。设想一下，如果tcp层没有keepalive的机制，一旦一方断开连接却没有发送FIN给另外一方的话，那么另外一方会一直以为这个连接还是存活的，几天，几月。那么这对服务器资源的影响是很大的。
+ HTTP的**keep-alive**一般我们都会带上中间的**横杠**，普通的http连接是客户端连接上服务端，然后结束请求后，由客户端或者服务端进行http连接的关闭。下次再发送请求的时候，客户端再发起一个连接，传送数据，关闭连接。这么个流程反复。但是一旦客户端发送connection:keep-alive头给服务端，且服务端也接受这个keep-alive的话，两边对上暗号，这个连接就可以复用了，一个http处理完之后，另外一个http数据直接从这个连接走了。减少新建和断开TCP连接的消耗。

二者的作用简单来说：

+ HTTP协议的Keep-Alive意图在于短时间内连接复用，希望可以短时间内在同一个连接上进行多次请求/响应。
+ TCP的KeepAlive机制意图在于保活、心跳，检测连接错误。当一个TCP连接两端长时间没有数据传输时(通常默认配置是2小时)，发送keepalive探针，探测链接是否存活。

##### 定制化

+ keepAliveTimeOut：多少毫秒后不响应的断开keepalive
+ maxKeepAliveRequests：多少次请求后keepalive断开失效
+ 使用`WebServerFactoryCustomizer<ConfigurableServletWebServerFactory>`定制化内嵌Tomcat配置

##### 配置类

```java
/**
 * 当Spring容器中没有TomcatEmbeddedServletContainerFactory这个bean时，会把此bean加载到Spring容器中
 */
@Component
public class WebServerConfiguration implements WebServerFactoryCustomizer<ConfigurableServletWebServerFactory> {
    @Override
    public void customize(ConfigurableServletWebServerFactory factory) {
        //使用对应工厂类提供给我们的接口定制化我们的tomcat connector
        ((TomcatServletWebServerFactory) factory).addConnectorCustomizers(new TomcatConnectorCustomizer() {
            @Override
            public void customize(Connector connector) {
                Http11NioProtocol protocol = (Http11NioProtocol) connector.getProtocolHandler();

                //定制化keepalivetimeout,设置30秒内没有请求则服务端自动断开keepalive链接
                protocol.setKeepAliveTimeout(30000);
                //当客户端发送超过10000个请求则自动断开keepalive链接
                protocol.setMaxKeepAliveRequests(10000);
            }
        });
    }
}
```

> 在customize方法里，可以利用Http11NioProtocol来设置各种参数，包括之前提到的默认内嵌Tomcat配置等。

#### 单Web容器上限

+ 线程数量：4核CPU 8G内存单进度调度线程数800~1000以上后将花费大量的时间在CPU调度上
+ 等待队列长度：队列做缓冲池用，但也不能无限长，消耗内存，出队入队也耗费CPU

#### MySql数据库QPS容量问题

+ 主键查询：千万级别数据=1~10毫秒
+ 唯一索引查询：千万级别数据=10~100毫秒
+ 非唯一索引查询：千万级别数据=100~1000毫秒
+ 无索引：百万条数据=1000毫秒+

#### MySql数据库TPS容量问题

+ 非插入更新删除操作：同查询，看where字段走的是哪种查询
+ 插入操作：1w~10w tps（依赖配置优化）

### 单机问题的解决方案（水平扩展）

以下为解决方案：

<img src="../pic/90.jpg" style="zoom:67%;" />

我们将nginx服务器用于三个方面：

+ 作为web服务器
+ 作为动静分离服务器
+ 作为反向代理服务器

对于前端的请求有两种，一种是静态资源的请求，一种是Ajax动态请求。对于静态资源的请求交由nginx服务器来处理，即静态资源放在nginx服务器上；对于动态请求，通过nginx反向代理来均衡到两台服务器上，这两台服务器处理处理这些动态请求，并且数据库的操作交由另一台数据库服务器来处理。

## 分布式扩展

### 配置nginx

这里使用的是openresty，将压缩包下载并解压。

首先安装必要的依赖：

```shell
yum install pcre-devel openssl-devel gcc curl
```

后进行安装：

```shell
./configure
make
make install
```

之后openresty会安装到/usr/local/openresty目录下。

在/usr/local/openresty/nginx目录下启动nginx测试：

```shell
sbin/nginx -c conf/nginx.conf
```

> 修改配置后可以使用`sbin/nginx -s reload`无缝重启

可以查看conf/nginx.conf，端口为80，用浏览器访问浏览器，有以下结果：

![](../pic/91png)

### nginx Web服务器

将前端代码scp到nginx服务器上的/usr/local/openresty/nginx/html目录下，我们在nginx.conf的文件中发现以下代码：

```
server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   html;
            index  index.html index.htm;
        }
        #省略
}
```

有以下信息：

+ location节点path：指定url映射key
+ location节点内容：root指定location path后对应的根路径，index指定默认的访问页

nginx服务器监听在80端口上，将/映射到html文件夹下，因此在浏览器中直接输入nginx/login.html就能访问到我们之前拷贝到服务器上的前端代码了：

![](../pic/92.png)

根据之前的架构图，我们应该将/resources定位到nginx目录的磁盘上的静态文件，因此需要修改nginx.conf文件：

修改如下，将/resources/路径映射到本地的nginx目录下的/html/resources/上，因此需要新建一个resources文件夹，然后将所有的静态文件转移到/resources目录下。

```
location /resources/ {
	alias       /usr/local/openresty/nginx/html/resources/;
	index  index.html index.htm;
}
```

使用新的路径访问：miaoshaserver/resources/login.html：

![](../pic/94.png)

### nginx 动静分离服务器

#### nginx配置

修改nginx.conf文件：

    upstream backend_server{
        server miaoshaserver1 weight=1;
        server miaoshaserver2 weight=1;
    }
    
    server {
        listen       80;
        server_name  localhost;
    
        #charset koi8-r;
    
        #access_log  logs/host.access.log  main;
    
        location /resources/ {
            alias       /usr/local/openresty/nginx/html/resources/;
            index  index.html index.htm;
        }
    
        location / {
            proxy_pass http://backend_server;
            proxy_set_header Host $http_host:$proxy_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
    }
首先添加**upstream**节点，设置权重**weight**。然后在/resources/的location节点后面再添加一个location节点（注意先后顺序，这将影响匹配的节点）。

**proxy_pass**字段填写需要转发的地址，**proxy_set_header**字段用于设置请求头。

如果后端服务器想知道用户的真实IP（因为默认下是nginx代理服务器发请求到后端服务器，后端服务器只能知道代理服务器的IP），我们必须配置**proxy_set_header**字段：

+ **proxy_set_header Host \$http_host:\$proxy_port**：

  设置Host为用户的真实IP与端口。

+ **proxy_set_header X-Real-IP $remote_addr**：

  X-Real-IP 代表的是客户端请求真实的 IP 地址。

+ **proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for**：

  X-Forwarded-For 记录着从客户端发起请求后访问过的每一个 IP 地址，当然第一个是发起请求的客户端本身的地址，各 IP 地址间由“英文逗号+空格”(`,`)分隔。

然后重启nginx服务器。

#### 后端服务器开启日志

在两个后端服务器的/www/miaosha路径下新建文件夹tomcat存放log日志。

在application.properties添加以下配置：

```properties
server.tomcat.accesslog.enabled=true
server.tomcat.accesslog.directory=/www/miaosha/tomcat
server.tomcat.accesslog.pattern=%h %l %u %t "%r" %s %b %D
```

pattern的含义：

+ `%h`：远端请求的IP地址
+ `%l`：
+ `%u`：远端主机的user
+ `%t`：时间
+ `"%r"`：http请求的第一行，例如：GET /item/list HTTP/1.0
+ `%s`：http返回状态码
+ `%b`：response的大小
+ `%D`：处理时长

例如：

```
172.22.228.146 - - [31/Mar/2021:21:50:39 +0800] "GET /item/list HTTP/1.0" 200 883 1089
```

#### 验证

现在可以访问浏览器，在两个后端服务器的tomcat日志中可以看到各自被转发的次数。

### 压测对比

服务器配置如下，一台用于nginx，两台用于后端服务器，一台用于数据库服务器。nginx服务器和数据库服务器的带宽都是10Mbps。

![](../pic/100.png)

#### 单机压测

首先在数据库服务器上进行单机压测，数据库和web都部署在同一台服务器。

压测参数为：

+ 线程数：1000
+ ramp-up时间：10
+ 循环次数：40

压测的过程中，单机上的状态：

<img src="../pic/96.png" style="zoom:67%;" />

发现占用cpu的大头是mysql进程。

压测结果：

![](../pic/97.png)

tps结果，常态在1900左右。

<img src="../pic/98.png" style="zoom:67%;" />

#### 分布式压测

压测的ip转向nginx服务器，压测参数与单机测试一致：

压测参数为：

+ 线程数：1000
+ ramp-up时间：10
+ 循环次数：40

压测结果：

![](../pic/99.png)

tps常态在2000左右。

<img src="../pic/101.png" style="zoom:67%;" />

#### 问题

下图是断开连接的四次挥手，我们可以看到，哪边主动断开连接（发送FIN报文），哪边将会进入到TIME-WAIT状态。

<img src="../pic/9.png" alt="img" style="zoom: 80%;" />

上面的图中的客户端和服务端这两个名字有歧义，可以单纯地把它们看做两个不同的主机，只是告诉我们哪边主动断开哪边就会进入到TIME-WAIT状态。

一般的，对于短连接，服务器会主动断开连接，因此服务器端会出现大量的TIME-WAIT状态。

Nginx upstream与后端的连接默认为短连接，而Nginx与前端的连接默认为长连接。因此Nginx服务器在与客户端通信时，长连接占大头，我们可以使用以下命令统计各种状态的数量：

```shell
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
```

现在用jmeter进行压测，分别在nginx服务器、后端服务器上使用命令查看各种状态的数量：

nginx服务器：

![](../pic/102.png)

后端服务器：

![](../pic/103.png)

我们发现后端服务器上有大量的TIME-WAIT状态的TCP连接。

如果Nginx只是作为reverse proxy的话，可能一个用户连接就需要多个向后端的短连接，并且由于是秒杀系统，用户会进行频繁地操作，连接的频繁建立与关闭将会带来很大的消耗。

因此我们需要配置nginx服务器到后端服务器的长连接，但是如果建立长连接的话，后端认为是长连接而不会主动关闭连接一般有个空闲超时，关闭连接由nginx服务器来做了，所以 nginx服务器上可能会出现大量的 TIME_WAIT，有利也有弊。

> keepalive如果偏大的话会影响短连接的处理，长连接存在时间长，容易快速消耗资源。

> 注意，这里说的是http的keep-alive，我们是通过统计TIME-WAIT状态的TCP连接的数量来验证的。
>
> http keep-alive是为了让tcp活得更久一点，以便在同一个连接上传送多个http，提高socket的效率。而tcp keep-alive是TCP的一种检测TCP连接状况的保鲜机制，类似心跳包。

#### 开启keepalive

修改conf/nginx.conf文件：

    upstream backend_server{
    	server miaoshaserver1 weight=1;
    	server miaoshaserver2 weight=1;
    	keepalive 250;
    }
    location / {
        proxy_pass http://backend_server;
        # proxy_set_header Host $http_host:$proxy_port;
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }

注意upstream处的keepalive的含义不是开启、关闭长连接的开关；也不是用来设置超时的timeout；更不是设置长连接池最大连接数。

官方解释：

+ 设置到upstream服务器的空闲keepalive连接的最大数量

+ 当这个数量被突破时，最近使用最少的连接将被关闭

+ 特别提醒：keepalive指令不会限制一个nginx worker进程到upstream服务器连接的总数量。

>根据官方的解释，容易清楚导致 nginx端出现大量TIME_WAIT的原因：
>
>keepalive设置的比较小（空闲数太小），导致高并发下nginx会频繁出现连接数震荡（超过该值会关闭连接），不停的关闭、开启和后端server保持的keepalive长连接。

重启后进行压测，nginx服务器和后端服务器的状态如下：

![](../pic/108.png)

![](../pic/105.png)

可以发现与之前不同的是，后端服务器的大量的TIME_WAIT状态转移到了nginx服务器上，但是相反地，由于长连接的复用，nginx服务器上的TIME_WAIT远远比开启keepalive前后端服务器上的状态少（5000）。

####  开启keepalive后的压测

![](../pic/106.png)

<img src="../pic/107.png" style="zoom:80%;" />

为什么吞吐量和TPS与未开启前差不多甚至更弱了点。。。。。。

但是90、95百分位的用时要比未开启的短。

## 分布式会话

### 为什么要引入分布式会话

<img src="../pic/90.jpg" style="zoom:67%;" />

在上图中，假设某位用户的登录请求被转发到了后端服务器A，那么session保存在A上，之后的某些需要session的操作请求被转发到了后端服务器B，而服务器B上并没有对应的session，因此需要用户重新进行登录。

因此我们需要引入分布式会话。

### 概况

#### 会话管理

+ 基于cookie传输sessionid：java tomcat容器session实现
+ 基于token传输sessionid：java代码session实现

#### 分布式会话

+ 基于cookie传输sessionid：java tomcat容器session实现迁移到redis
+ 基于cookie传输类似sessionid：java代码session实现迁移到redis

### 基于cookie的实现

#### 配置redis

redis安装在mysql服务器上。

因为安装在opt目录下，因此更改redis安装目录的所属为当前用户：

```shell
sudo chown $USER:$USER /opt/redis-6.2.1
```

配置日志，编辑redis.conf文件，找到**logfile**字段，设置log文件位置，然后用`redis-server redis.conf`的方式启动。

将protected-mode字段为no，允许远程连接。

将自己的内网地址bind上。

将默认端口更改，将requirepass字段设置密码。

redis-cli -h ip -p port 连接测试。

使用auth 密码授权。

> 停止redis可以使用redis-cli的shutdown命令。

#### pom.xml引入依赖

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.session</groupId>
    <artifactId>spring-session-data-redis</artifactId>
</dependency>
```

#### RedisConfig

```java
@Component
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)
public class RedisConfig {
}
```

#### application.yaml

这里的host是用于本地测试的，需要改成redis所在的服务器地址。

```yaml
spring:
  redis:
    host: 127.0.0.1
    port: 6379
    database: 10
    jedis:
      pool:
        max-active: 50
        min-idle: 20
```

#### 相关的实体类序列化

因为需要将session保存到redis中，因此被set的model需要实现`Serializable`接口。

### 基于token的实现

基于cookie的实现方式是调用`httpServletRequest.getSession().setAttribute`的方法。

现在我们将信息基于token存储在redis中。

例如登录成功后，生成唯一的UUID作为redis中的key，将userModel作为value存入redis中：

```java
@Autowired
private RedisTemplate redisTemplate;
```

```java
//将登录凭证加入到用户登录成功的session内
//this.httpServletRequest.getSession().setAttribute("IS_LOGIN", true);
//this.httpServletRequest.getSession().setAttribute("LOGIN_USER", userModel);
//修改成若用户登录验证成功后将对应的登录信息和登录凭证一起存入redis中

//生成登录凭证token，UUID
String uuidToken = UUID.randomUUID().toString();
uuidToken = uuidToken.replace("-", "");
//建立token和用户登陆态之间的联系
redisTemplate.opsForValue().set(uuidToken,userModel);
redisTemplate.expire(uuidToken,1, TimeUnit.HOURS);
//下发token
return CommonReturnType.create(uuidToken);
```

前端把token存储下来，当用户下单时将token传到后端，后端根据token从redis中取出对应的userModel，进行下单的数据库操作：

```java
//Boolean isLogin = (Boolean) httpServletRequest.getSession().getAttribute("IS_LOGIN");
String token = httpServletRequest.getParameterMap().get("token")[0];
if (StringUtils.isEmpty(token)) {
    throw new BusinessException(EmBusinessError.USER_NOT_LOGIN, "用户尚未登录");
}
//获取用户登录信息
UserModel userModel = (UserModel) redisTemplate.opsForValue().get(token);
if (userModel == null) {
    throw new BusinessException(EmBusinessError.USER_NOT_LOGIN, "用户尚未登录");
}
//UserModel userModel = (UserModel) httpServletRequest.getSession().getAttribute("LOGIN_USER");
OrderModel orderModel = orderService.createOrder(userModel.getId(), itemId, amount, promoId);
```

## 多级缓存

### 缓存设计

+ 用快速存取设备
+ 将缓存推到离用户最近的地方
+ 脏缓存清理

### 多级缓存

+ redis缓存
+ 热点内存本地缓存
+ nginx proxy cache缓存
+ nginx lua缓存

### redis缓存

redis作为集中式缓存：

<img src="../pic/109.jpg" style="zoom:67%;" />

#### 单机版缺陷

容量问题，单点问题。

#### sentinal哨兵模式

主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑**哨兵模式**。

哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是**哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。**

![img](../pic/110.jpg)

这里的哨兵有两个作用：

- 通过心跳机制，让Redis服务器返回其运行状态，包括主服务器和从服务器。
- 当哨兵监测到master宕机，会自动将slave切换成master，然后通过**发布订阅模式**通知其他的从服务器，修改配置文件，让它们切换主机。

假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为**主观下线**。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为**客观下线**。这样对于客户端而言，一切都是透明的。

#### 集群cluster模式

### redis集中式缓存动态详情页

在itemController中更改getItem方法的逻辑：

```java
//商品详情页
@RequestMapping(value = "/get", method = {RequestMethod.GET})
@ResponseBody
public CommonReturnType getItem(@RequestParam(name = "id") Integer id) {
    //根据商品id到redis内获取
    ItemModel itemModel = (ItemModel) redisTemplate.opsForValue().get("item_" + id);
    if (itemModel == null) {
        itemModel = itemService.getItemById(id);
        //缓存
        redisTemplate.opsForValue().set("item_" + id, itemModel);
        redisTemplate.expire("item_"+id, 10, TimeUnit.MINUTES);
    }
    ItemVO itemVO = convertFromItemModel(itemModel);
    return CommonReturnType.create(itemVO);
}
```

进行测试后我们发现，存储在redis中的数据并不友好，可以考虑序列化成json串：

```java
@Component
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)
public class RedisConfig {

    @Bean
    public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) {
        RedisTemplate redisTemplate = new RedisTemplate();
        redisTemplate.setConnectionFactory(redisConnectionFactory);
        //首先解决key的序列化方式
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        redisTemplate.setKeySerializer(stringRedisSerializer);
        //解决value的序列化方式
        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);
        redisTemplate.setValueSerializer(jackson2JsonRedisSerializer);
        return redisTemplate;
    }
}
```

结果如下：

![](/home/yikang/Picture/2021-04-02 19-39-55屏幕截图.png)

发现，时间这个属性序列化得并不好，我们需要为他定制一个序列化与反序列化方案：

序列化方案：

```java
public class JodaDateTimeJsonSerializer extends JsonSerializer<DateTime> {
    @Override
    public void serialize(DateTime dateTime, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException {
        jsonGenerator.writeString(dateTime.toString("yyyy-MM-dd HH:mm:ss"));
    }
}
```

反序列化方案：

```java
public class JodaDateTimeJsonDeserializer extends JsonDeserializer<DateTime> {
    @Override
    public DateTime deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException, JsonProcessingException {
        String dateString =jsonParser.readValueAs(String.class);
        DateTimeFormatter formatter = DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss");

        return DateTime.parse(dateString,formatter);
    }
}
```

将这些方案应用到RedisConfig中：

```java
@Component
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)
public class RedisConfig {

    @Bean
    public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) {
        RedisTemplate redisTemplate = new RedisTemplate();
        redisTemplate.setConnectionFactory(redisConnectionFactory);
        //首先解决key的序列化方式
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        redisTemplate.setKeySerializer(stringRedisSerializer);
        //解决value的序列化方式
        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);
        //为joda.time.DateTime定制序列化与反序列化
        ObjectMapper objectMapper = new ObjectMapper();
        SimpleModule simpleModule = new SimpleModule();
        simpleModule.addSerializer(DateTime.class, new JodaDateTimeJsonSerializer());
        simpleModule.addDeserializer(DateTime.class, new JodaDateTimeJsonDeserializer());
        objectMapper.registerModule(simpleModule);
        jackson2JsonRedisSerializer.setObjectMapper(objectMapper);
        redisTemplate.setValueSerializer(jackson2JsonRedisSerializer);
        return redisTemplate;
    }
}
```

现在redis中存储的数据的格式已经非常的简洁了：

![2021-04-05 15-26-27屏幕截图](../pic/111.png)

但是还有一个问题，进行浏览器访问时，从redis中取出数据强制装换成`UserModel`时出错：

![](../pic/112.png)

这时我们需要调用`objectMapper.activateDefaultTyping`，它会在序列化后的数据添加上类的信息，用于之后的反序列化的强制转换。

```java
@Component
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)
public class RedisConfig {

    @Bean
    public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) {
        RedisTemplate redisTemplate = new RedisTemplate();
        redisTemplate.setConnectionFactory(redisConnectionFactory);
        //首先解决key的序列化方式
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        redisTemplate.setKeySerializer(stringRedisSerializer);
        //解决value的序列化方式
        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);
        //为joda.time.DateTime定制序列化与反序列化
        ObjectMapper objectMapper = new ObjectMapper();
        SimpleModule simpleModule = new SimpleModule();
        simpleModule.addSerializer(DateTime.class, new JodaDateTimeJsonSerializer());
        simpleModule.addDeserializer(DateTime.class, new JodaDateTimeJsonDeserializer());

        objectMapper.activateDefaultTyping(objectMapper.getPolymorphicTypeValidator(), ObjectMapper.DefaultTyping.NON_FINAL);

        objectMapper.registerModule(simpleModule);
        jackson2JsonRedisSerializer.setObjectMapper(objectMapper);
        redisTemplate.setValueSerializer(jackson2JsonRedisSerializer);
        return redisTemplate;
    }
}
```

在redis中的数据添加上了类的信息：

![](../pic/113.png)

最终该功能成功实现。

### 本地热点缓存

+ 热点数据
+ 脏读不敏感
+ 内存可控

#### Guava cache

+ 可控制的大小和超时时间
+ 可配置的lru策略
+ 线程安全

#### 本地热点缓存实现

使用google提供的guava来实现本地热点缓存。

在pom文件加入以下依赖：

```xml
<!-- https://mvnrepository.com/artifact/com.google.guava/guava -->
<dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version>18.0</version>
</dependency>
```

实现本地缓存的service：

```java
//封装本地缓存操作类
public interface CacheService {
    //存方法
    void setCommonCache(String key, Object value);

    //取方法
    Object getFromCommonCache(String key);
}

@Service
public class CacheServiceImpl implements CacheService {

    private Cache<String, Object> commonCache = null;

    @PostConstruct
    public void init() {
        commonCache = CacheBuilder.newBuilder()
                //设置缓存容器的初始容量为10
                .initialCapacity(10)
                //设置缓存中最大可以存储100个KEY,超过100个之后会按照LRU的策略移除缓存项
                .maximumSize(100)
                //设置写缓存后多少秒过期
                //设置成AfterAccess不合理，因为热点数据是频繁被访问的，那么将无法过期
                .expireAfterWrite(60, TimeUnit.SECONDS).build();
    }

    @Override
    public void setCommonCache(String key, Object value) {
        commonCache.put(key, value);
    }

    @Override
    public Object getFromCommonCache(String key) {
        return commonCache.getIfPresent(key);
    }
}
```

在itemController的getItem方法里使用本地缓存，如果本地缓存未命中，则使用redis缓存，如果redis缓存未命中，那么从数据库中查找：

```java
@Autowired
private CacheService cacheService;

//商品详情页
@RequestMapping(value = "/get", method = {RequestMethod.GET})
@ResponseBody
public CommonReturnType getItem(@RequestParam(name = "id") Integer id) {
    String key = "item_" + id;
    //先从本地缓存取
    ItemModel itemModel = (ItemModel) cacheService.getFromCommonCache(key);
    if (itemModel == null) {
        //本地缓存未命中，那么到redis中取
        itemModel = (ItemModel) redisTemplate.opsForValue().get(key);
        if (itemModel == null) {
            itemModel = itemService.getItemById(id);
            //redis缓存
            redisTemplate.opsForValue().set(key, itemModel);
            redisTemplate.expire(key, 10, TimeUnit.MINUTES);
        }
        //本地缓存
        cacheService.setCommonCache(key, itemModel);
    }
    //根据商品id到redis内获取
    ItemVO itemVO = convertFromItemModel(itemModel);
    return CommonReturnType.create(itemVO);
}
```

#### 压测

之前可能因为是带宽的原因，压测不理想，因此现在更新了服务器的带宽，现在的配置如下：

![](../pic/114.png)

现在将nginx服务器的带宽由10Mbps升到了20Mbps，两个后端服务器由5Mbps升到了10Mbps，将带宽带来的影响尽可能地降低。

因为没有保存之前版本的jar包，因此这里只比较redis缓存版本与本地热点缓存+redis缓存的版本的压测结果。

压测参数与之前是一样的：

![](../pic/115.png)

首先是redis缓存版本的压测结果：

![](../pic/116.png)

![](../pic/117.png)

之后是本地热点缓存版本的压测结果：

![](../pic/118.png)

![](../pic/119.png)

发现使用本地热点缓存，无论是用时还是吞吐量还是tps，新版本的性能要远远优于旧版本。

### nginx proxy cache

#### 简介

+ nginx反向代理前置
+ 依靠文件系统存索引级的文件
+ 依靠内存缓存文件地址

#### 实现

修改nginx.conf文件：

```
	#声明一个cache缓存节点的内容
	#缓存的地址为/usr/local/openresty/nginx/tmp_cache
	#levels=1:2表示建立二级索引
	#keys_zone=tmp_cache:100m类似于命名空间，100m的上限，如果超出则将进行LRU淘汰
	#inactive=7d表示7天的有效
	#max_size=10g是指总的大小
    proxy_cache_path /usr/local/openresty/nginx/tmp_cache levels=1:2 keys_zone=tmp_cache:100m inactive=7d max_size=10g;
    server {
    	...
    	location / {
	    proxy_pass http://backend_server;
	    #指定cache
	    proxy_cache tmp_cache;
	    #用uri作为key
	    proxy_cache_key $uri;
	    #只有这些状态码的请求结果会被缓存，404等错误码的结果不能缓存，否则一旦出现404，那么在有效期内永远都是404
	    proxy_cache_valid 200 206 304 302 7d;
	    #
	    # proxy_set_header Host $http_host:$proxy_port;
	    proxy_set_header Host $http_host;
	    proxy_set_header X-Real-IP $remote_addr;
	    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	    proxy_http_version 1.1;
	    proxy_set_header Connection "";
		}
    }
```

重启nginx，访问uri，可以看到在/tmp_cache/8/f6下生成了缓存：

<img src="../pic/120.png" style="zoom:80%;" />

#### 压测

![](../pic/121.png)

![](../pic/122.png)

发现使用nginx proxy cache后，不管是用时、吞吐量还是tps，性能反而下降了？

这是因为nginx proxy cache并没有把缓存保存到内存，而是保存到本地文件系统中，因此读取本地文件的速度还不如反向代理到后端服务器通过热点缓存读取的速度。

### nginx lua挂载点

+ **init_by_lua**：系统启动时调用
+ **init_worker_by_lua**：worker进程启动时调用
+ **set_by_lua**：变量用复杂lua return
+ **rewrite_by_lua**：重写url规则
+ **access_by_lua**：权限验证阶段
+ **content_by_lua**：内容输出节点

### nginx shared dic

nginx的共享**内存**，称为**共享字典**项，对于所有的worker进程都可见，是一种全局变量, LRU淘汰。

这里使用ngxin shared dic的数据结构来缓存数据。

首先编写lua脚本：

```lua
function get_from_cache(key)
	local cache_ngx = ngx.shared.my_cache
	local value = cache_ngx:get(key)
	return value
end

function set_to_cache(key, value, expire)
	if not expire then
		expire = 0
	end
	local cache_ngx = ngx.shared.my_cache
	local succ, err, forcible = cache_ngx:set(key, value, expire)
	return succ
end

local args = ngx.req.get_uri_args()
local id = args["id"]
local item_model = get_from_cache("item_"..id)
if item_model == nil then
	local resp = ngx.location.capture("/item/get?id="..id)
	item_model = resp.body
	set_to_cache("item_"..id, item_model, 1*60)
end
ngx.say(item_model)
```

其中，`ngx.shared.my_cache`引用的是nginx.conf文件中的定义的共享字典：

```
lua_shared_dict my_cache 128m;
    server {
    ......
	}
```

还需要在nginx.conf定义一个location节点：

```
location /luaitem/get{
	    default_type "application/json";
	    content_by_lua_file ../lua/itemsharedic.lua;
}
```

这里的路径定义为`/luaitem/get`只是简单地做个测试。

这样就实现了nginx shared dic的本地缓存了。

但这样的方式使得压力全部在nginx服务器上了，并且脏读的现象不好处理，更新机制不方便。

### nginx连接redis

编写lua脚本：

```lua
local log = ngx.log
local ERR = ngx.ERR
local function errlog(...)
	log(ERR, "Redis:", ...)
end
local args = ngx.req.get_uri_args()
local id = args["id"]
local redis = require "resty.redis"
local cache = redis:new()
local ok, err = cache:connect("172.22.228.151", 6789)
local ok2, err2 = cache:auth("1131487340")
if not ok2 then
	errlog("failed to auth", err2)	
	return
end
local item_model = cache:get("item_"..id)
if item_model == ngx.null or item_model == nil then
	local resp = ngx.location.capture("/item/get?id="..id)
	item_model = resp.body
end

ngx.say(item_model)
```

修改nginx.conf：

```
location /luaitem/get{
	    default_type "application/json";
	    content_by_lua_file ../lua/itemredis.lua;
}
```

这样就能将nginx将部分业务直接请求redis。

可以考虑再添加一台redis服务器，用作从服务器，从服务器负责读，而nginx只处理读的请求，写的请求转发到后端服务器到主redis服务器上处理。

那么新的架构如下：

<img src="../pic/124.jpg" style="zoom:67%;" />